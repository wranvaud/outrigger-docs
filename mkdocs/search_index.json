{
    "docs": [
        {
            "location": "/",
            "text": "Outrigger\n\n\nEasy containerized development environments based on Docker\n\n\nThe various projects within the Outrigger ecosystem provide simple to manage\nproject environments for any technology and deployment architecture. These\nenvironments are based upon Docker and Outrigger helps you tie the pieces\ntogether to provide a stable foundation for project work that includes:\n\n\n\n\nA Docker Machine based VM\n\n\nDNS services\n\n\nNetwork routing\n\n\nHigh speed filesystems\n\n\nPersistent data storage\n\n\n\n\nThe Outrigger approach is to configure and use standard Docker tools. This allows\nyou the option to learn these tools when you want to and provides the\nflexibility to override or substitute components should it be necessary for your\nproject.\n\n\nIf you are brand new to Outrigger and Docker, it is \nhighly recommended\n you read \nthe \nGlossary\n and \nArchitecture\n \ndocuments to get familiar with the concepts and terminology used throughout this \ndocumentation.",
            "title": "Home"
        },
        {
            "location": "/#outrigger",
            "text": "Easy containerized development environments based on Docker  The various projects within the Outrigger ecosystem provide simple to manage\nproject environments for any technology and deployment architecture. These\nenvironments are based upon Docker and Outrigger helps you tie the pieces\ntogether to provide a stable foundation for project work that includes:   A Docker Machine based VM  DNS services  Network routing  High speed filesystems  Persistent data storage   The Outrigger approach is to configure and use standard Docker tools. This allows\nyou the option to learn these tools when you want to and provides the\nflexibility to override or substitute components should it be necessary for your\nproject.  If you are brand new to Outrigger and Docker, it is  highly recommended  you read \nthe  Glossary  and  Architecture  \ndocuments to get familiar with the concepts and terminology used throughout this \ndocumentation.",
            "title": "Outrigger"
        },
        {
            "location": "/getting-started/system-requirements/",
            "text": "System Requirements\n\n\nHardware\n\n\nWe recommend 16 GB of RAM when working with containerization / virtualization. You can certainly\nget by with 8 GB but you may want to keep other applications to a minimum. It is also dependent on\nthe RAM configuration required if using virtualization. By default the Outrigger Docker Host is\nconfigured to run with 4 GB of RAM.\n\n\nSoftware\n\n\nMac\n\n\nHomebrew\n is also required for package installation.\n\n\nFor Virtualization you can choose one of the following\n\n\n\n\nVirtualBox 5.1.x\n (the default supported option)\n\n\nVMWare Fusion\n\n\nXhyve\n with \nDocker Machine Xhyve\n\n\n\n\nWindows\n\n\nVirtualBox is required.\n\n\nLinux\n\n\nDocker is required to be running as a system service.",
            "title": "System Requirements"
        },
        {
            "location": "/getting-started/system-requirements/#system-requirements",
            "text": "",
            "title": "System Requirements"
        },
        {
            "location": "/getting-started/system-requirements/#hardware",
            "text": "We recommend 16 GB of RAM when working with containerization / virtualization. You can certainly\nget by with 8 GB but you may want to keep other applications to a minimum. It is also dependent on\nthe RAM configuration required if using virtualization. By default the Outrigger Docker Host is\nconfigured to run with 4 GB of RAM.",
            "title": "Hardware"
        },
        {
            "location": "/getting-started/system-requirements/#software",
            "text": "",
            "title": "Software"
        },
        {
            "location": "/getting-started/system-requirements/#mac",
            "text": "Homebrew  is also required for package installation.  For Virtualization you can choose one of the following   VirtualBox 5.1.x  (the default supported option)  VMWare Fusion  Xhyve  with  Docker Machine Xhyve",
            "title": "Mac"
        },
        {
            "location": "/getting-started/system-requirements/#windows",
            "text": "VirtualBox is required.",
            "title": "Windows"
        },
        {
            "location": "/getting-started/system-requirements/#linux",
            "text": "Docker is required to be running as a system service.",
            "title": "Linux"
        },
        {
            "location": "/getting-started/mac-installation/",
            "text": "Installation\n\n\nMac Installation\n\n\nInstall VirtualBox\n\n\nVirtualBox Downloads\n\n\nInstall Homebrew\n\n\nHomebrew Website\n\n\nIf Homebrew is already installed, then be sure to do a \nbrew update\n \n\n\nTap the Outrigger repository\n\n\nbrew tap phase2/outrigger\n\n\nInstall Outrigger (and dependencies)\n\n\nbrew install phase2/outrigger/rig\n\n\nThis will install the \nrig\n binary as well the Docker and other dependencies.\n\n\nCreate the Docker Host\n\n\nOnce everything checks out, run the following command to create a new docker host. \n(You will likely be prompted for your admin password)\n\n\nrig start\n\n\nHere are some configuration options available to you to customize your setup:\n\n\nOptions on \nrig\n:\n\n\n\n\nname:\n The Docker Machine name for the VM. Defaults to \ndev\n\n\n\n\nOptions on the \nstart\n command:\n\n\n\n\ndriver:\n The driver to create the Docker Machine with. Choices are:\n\n\nvirtualbox\n - default\n\n\nvmwarefusion\n\n\nxhyve\n\n\n\n\n\n\ncpuCount:\n The number of virtual CPU you want to allocate to this VM. Defaults to 2\n\n\nmemSize:\n The size memory you want to configure for this VM, in Megabytes. Defaults to 4096\n\n\ndiskSize:\n The size drive you want to configure for this VM, in Gigabytes. Defaults to 40\n\n\n\n\nConfigure your shell to set Outrigger Docker environment\n\n\nTo configure the shell with the proper Outrigger environment, run the following command\nafter the docker host has started from the previous step.\n\n\neval \"$(rig config)\"\n\n\nFor convenience, you should make this automatic on every terminal you launch. To do that \nadd the following to your \n.bash_profile\n, \n.zshrc\n or equivalent:\n\n\n# Support for Outrigger\neval \n$(rig config)\n\nalias re='eval \n$(rig config)\n'\n\n\n\n\n\n\nRunning rig config\n\n\nEven with automatic execution in your shell, this command must be run in any existing \nterminal windows after \nrig start\n or \nrig restart\n commands. To support that\nsee the \nre\n alias in the shell init file above. Just run \nre\n after a \nrig start\n\n\n\n\nSupport for Docker for Mac\n\n\nSee \nDocker for Mac Support",
            "title": "Mac Installation"
        },
        {
            "location": "/getting-started/mac-installation/#installation",
            "text": "",
            "title": "Installation"
        },
        {
            "location": "/getting-started/mac-installation/#mac-installation",
            "text": "",
            "title": "Mac Installation"
        },
        {
            "location": "/getting-started/mac-installation/#install-virtualbox",
            "text": "VirtualBox Downloads",
            "title": "Install VirtualBox"
        },
        {
            "location": "/getting-started/mac-installation/#install-homebrew",
            "text": "Homebrew Website  If Homebrew is already installed, then be sure to do a  brew update",
            "title": "Install Homebrew"
        },
        {
            "location": "/getting-started/mac-installation/#tap-the-outrigger-repository",
            "text": "brew tap phase2/outrigger",
            "title": "Tap the Outrigger repository"
        },
        {
            "location": "/getting-started/mac-installation/#install-outrigger-and-dependencies",
            "text": "brew install phase2/outrigger/rig  This will install the  rig  binary as well the Docker and other dependencies.",
            "title": "Install Outrigger (and dependencies)"
        },
        {
            "location": "/getting-started/mac-installation/#create-the-docker-host",
            "text": "Once everything checks out, run the following command to create a new docker host. \n(You will likely be prompted for your admin password)  rig start  Here are some configuration options available to you to customize your setup:  Options on  rig :   name:  The Docker Machine name for the VM. Defaults to  dev   Options on the  start  command:   driver:  The driver to create the Docker Machine with. Choices are:  virtualbox  - default  vmwarefusion  xhyve    cpuCount:  The number of virtual CPU you want to allocate to this VM. Defaults to 2  memSize:  The size memory you want to configure for this VM, in Megabytes. Defaults to 4096  diskSize:  The size drive you want to configure for this VM, in Gigabytes. Defaults to 40",
            "title": "Create the Docker Host"
        },
        {
            "location": "/getting-started/mac-installation/#configure-your-shell-to-set-outrigger-docker-environment",
            "text": "To configure the shell with the proper Outrigger environment, run the following command\nafter the docker host has started from the previous step.  eval \"$(rig config)\"  For convenience, you should make this automatic on every terminal you launch. To do that \nadd the following to your  .bash_profile ,  .zshrc  or equivalent:  # Support for Outrigger\neval  $(rig config) \nalias re='eval  $(rig config) '   Running rig config  Even with automatic execution in your shell, this command must be run in any existing \nterminal windows after  rig start  or  rig restart  commands. To support that\nsee the  re  alias in the shell init file above. Just run  re  after a  rig start",
            "title": "Configure your shell to set Outrigger Docker environment"
        },
        {
            "location": "/getting-started/mac-installation/#support-for-docker-for-mac",
            "text": "See  Docker for Mac Support",
            "title": "Support for Docker for Mac"
        },
        {
            "location": "/getting-started/linux-installation/",
            "text": "Linux Installation\n\n\nWhen running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe \nhow to run Outrigger projects on Linux.\n\n\nLinux Requirements\n\n\n\n\nThe dnsdock container, used to support automatic creation and maintenance of DNS namespace for the containers\n\n\nUse of one of three options to forward DNS queries to the dnsdock container (see \n\nLinux DNS configuration options\n below)\n\n\n\n\nLinux installation on Fedora/Centos\n\n\n\n\nInstall Docker for Fedora\n or \n\nInstall Docker for CentOS\n\n\nInstall Docker Compose\n\n\nSet the DNS configuration for Docker\n\n\nWe need to modify the command that docker uses within systemd\n\n\nsudo mkdir /etc/systemd/system/docker.service.d\n\n\nsudo vi /etc/systemd/system/docker.service.d/docker.conf\n\n\nIn that file put something like the following:    \n\n\n\n\n\n\n\n\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// --dns=172.17.0.1\n\n\n\n\n\n\nSet up the docker0 network as trusted\n\n\nsudo firewall-cmd --zone=trusted --add-interface=docker0 \n sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent\n\n\n\n\n\n\nStart the docker daemon\n\n\nsudo systemctl start docker\n\n\n\n\n\n\n\n\nLinux installation on Ubuntu/Debian\n\n\n\n\nInstall Docker for Ubuntu\n or \n\nInstall Docker for Debian\n\n\nInstall Docker Compose\n\n\n\n\nIf using Upstart\n\n\n\n\nSet the DNS configuration for dnsdock, as well as known RFC-1918 address space\n\n\nPlease note that the following command will over-write your existing Docker daemon configuration file.  Please \nset the -dns=172.17.0.1 option manually as an alternative\n\n\necho 'DOCKER_OPTS=\"-dns=172.17.0.1\"' | sudo tee /etc/default/docker\n\n\n\n\n\n\nStart the docker daemon\n\n\nsudo start docker\n\n\n\n\n\n\n\n\nIf using Systemd\n\n\n\n\nSet the DNS configuration for Docker\n\n\nWe need to modify the command that docker uses within systemd\n\n\nsudo mkdir /etc/systemd/system/docker.service.d\n\n\nsudo vi /etc/systemd/system/docker.service.d/docker.conf\n\n\nIn that file put something like the following:    \n\n\n\n\n\n\n\n\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// --dns=172.17.0.1\n\n\n\n\nLinux DNS configuration options\n\n\nMethod 1: dnsmasq via NetworkManager\n\n\nThis method works well with no other needed software provided that you have unfettered access to your system's \nconfiguration, and are using NetworkManager to maintain your networking stack\n\n\n\n\nAdd the line dns=dnsmasq to /etc/NetworkManager/NetworkManager.conf under the [main] configuration stanza. This will \ncause NetworkManager to spawn and use a dnsmasq process for all name resolution. If you already have a local configuration, \nensure that it is not configured to start on system boot.\n\n\nAdd a single rule to direct all DNS lookups for .vm addresses to the 172.17.0.1 address.\n\n\necho 'server=/vm/172.17.0.1' | sudo tee /etc/NetworkManager/dnsmasq.d/dnsdock.conf\n\n\n\n\n\n\nRestart NetworkManager, either through systemd, or by simply rebooting.  To restart via systemd:\n\n\nsystemctl restart NetworkManager\n\n\n\n\n\n\nRun the dnsdock container\n\n\n\n\ndocker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm\n\n\n\n\nMethod 2: dnsdock as main resolver\n\n\nThis method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS \nserver. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the \nrest to your normal DNS server.\n\n\nThis example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1.\n\n\n\n\nRun the dnsdock container, specifying your upstream DNS server at the end.\n\n\n\n\ndocker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm\n\n\n\n\n\n\nConfigure 172.17.0.1 as your first DNS resolver in your network configuration. The method for doing this may differ \nbased on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as \nthe first 'nameserver' line in your /etc/resolv.conf file.\n\n\n\n\nMethod 3: libnss-resolver\n\n\nlibnss-resolver is an app that adds Mac-style /etc/resolver/$FQDN files to the Linux NSS resolution stack to query a \ndifferent DNS server for any .vm address.  It may be the easiest option for most installations.\n\n\nThere are releases for Fedora 20, Ubuntu 12.04 and Ubuntu 14.04.\n\n\n\n\nInstall \nlibnss-resolver\n\n\nSet up .vm hostname resolution\n\n\necho 'nameserver 172.17.0.1:53' | sudo tee /etc/resolver/vm\n\n\n\n\n\n\nRun the dnsdock container\n\n\n\n\ndocker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm\n\n\n\n\nRunning dnsdock as a service\n\n\n\n\nCreate the file \n/etc/systemd/system/dnsdock.service\n with the following contents\n\n\n\n\n[Unit]\nDescription=DNSDock\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill dnsdock\nExecStartPre=-/usr/bin/docker rm dnsdock\nExecStart=/usr/bin/docker run --rm --name dnsdock -v /var/run/docker.sock:/var/run/docker.sock -l com.dnsdock.name=dnsdock -l com.dnsdock.image=outrigger -p 172.17.0.1:53:53/udp aacebedo/dnsdock:v1.16.1-amd64  --domain=vm\nExecStop=/usr/bin/docker stop dnsdock\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\n\n\nEnsure Docker is registered with systemctl\n\n\nsystemctl enable docker\n\n\n\n\n\n\nRegister dnsdock service: \n\n\nsystemctl enable dnsdock \n systemctl daemon-reoload\n\n\n\n\n\n\nNow you can start/stop the dnsdock service with \n\n\nsystemctl [start|stop] dnsdock\n\n\n\n\n\n\nYou can also check its status \n\n\nsystemctl status dnsdock\n\n\n\n\n\n\n\n\nVerifying DNS is working\n\n\nOnce you have your environment set up, you can use the following tests to ensure things are running properly.\n\n\n\n\ndig @172.17.0.1 dnsdock.outrigger.vm.\n\n\nYou should get a 172.17.0.0/16 address\n\n\n\n\n\n\nping dnsdock.outrigger.vm\n\n\nYou should get echo replies from a 172.17.0.0/16 address\n\n\n\n\n\n\ngetent hosts dnsdock.outrigger.vm\n\n\nYou should get a 172.17.0.0/16 address\n\n\n\n\n\n\nVerify DNS works between containers\n\n\nOpen a shell for a second test container\n\n\ndocker run --rm -l com.dnsdock.name=test -l com.dnsdock.image=outrigger -it alpine sh\n\n\n\n\n\n\nFrom its prompt: \n\n\nping dnsdock.outrigger.vm\n\n\nYou should get echo replies from a 172.17.0.0/16 address\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nbind: address already in use\n\n\nMake sure you are not running a service which binds all mapped IPs.  For example,\n\n\n docker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm\n\n  6aa76d0df98ede7e01c1cef53f105a79a60bab72ee905a1acd76ad57d4aeb014\n  docker: Error response from daemon: driver failed programming external connectivity on endpoint dnsdock\n  (62b3788e1f60530d1f468b46833f0bf8227955557da3a135356184f5b7d57bde): Error starting userland proxy: listen\n  udp 172.17.0.1:53: bind: address already in use.\n\n\n\n\nIn this case the culprit was \nbind9/named\n was attaching to all interfaces (on port 53)\n\n\n\n\nThe solution\n\n\nsystemctl stop bind9\n\n\n\n\n\n\nTo avoid having to stop \nbind9\n every session\n\n\nsystemctl disable bind9\n\n\n\n\n\n\nYou may need to restart NetworkManager\n\n\nsystemctl restart NetworkManager",
            "title": "Linux Installation"
        },
        {
            "location": "/getting-started/linux-installation/#linux-installation",
            "text": "When running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe \nhow to run Outrigger projects on Linux.",
            "title": "Linux Installation"
        },
        {
            "location": "/getting-started/linux-installation/#linux-requirements",
            "text": "The dnsdock container, used to support automatic creation and maintenance of DNS namespace for the containers  Use of one of three options to forward DNS queries to the dnsdock container (see  Linux DNS configuration options  below)",
            "title": "Linux Requirements"
        },
        {
            "location": "/getting-started/linux-installation/#linux-installation-on-fedoracentos",
            "text": "Install Docker for Fedora  or  Install Docker for CentOS  Install Docker Compose  Set the DNS configuration for Docker  We need to modify the command that docker uses within systemd  sudo mkdir /etc/systemd/system/docker.service.d  sudo vi /etc/systemd/system/docker.service.d/docker.conf  In that file put something like the following:         [Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// --dns=172.17.0.1   Set up the docker0 network as trusted  sudo firewall-cmd --zone=trusted --add-interface=docker0   sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent    Start the docker daemon  sudo systemctl start docker",
            "title": "Linux installation on Fedora/Centos"
        },
        {
            "location": "/getting-started/linux-installation/#linux-installation-on-ubuntudebian",
            "text": "Install Docker for Ubuntu  or  Install Docker for Debian  Install Docker Compose   If using Upstart   Set the DNS configuration for dnsdock, as well as known RFC-1918 address space  Please note that the following command will over-write your existing Docker daemon configuration file.  Please \nset the -dns=172.17.0.1 option manually as an alternative  echo 'DOCKER_OPTS=\"-dns=172.17.0.1\"' | sudo tee /etc/default/docker    Start the docker daemon  sudo start docker     If using Systemd   Set the DNS configuration for Docker  We need to modify the command that docker uses within systemd  sudo mkdir /etc/systemd/system/docker.service.d  sudo vi /etc/systemd/system/docker.service.d/docker.conf  In that file put something like the following:         [Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// --dns=172.17.0.1",
            "title": "Linux installation on Ubuntu/Debian"
        },
        {
            "location": "/getting-started/linux-installation/#linux-dns-configuration-options",
            "text": "",
            "title": "Linux DNS configuration options"
        },
        {
            "location": "/getting-started/linux-installation/#method-1-dnsmasq-via-networkmanager",
            "text": "This method works well with no other needed software provided that you have unfettered access to your system's \nconfiguration, and are using NetworkManager to maintain your networking stack   Add the line dns=dnsmasq to /etc/NetworkManager/NetworkManager.conf under the [main] configuration stanza. This will \ncause NetworkManager to spawn and use a dnsmasq process for all name resolution. If you already have a local configuration, \nensure that it is not configured to start on system boot.  Add a single rule to direct all DNS lookups for .vm addresses to the 172.17.0.1 address.  echo 'server=/vm/172.17.0.1' | sudo tee /etc/NetworkManager/dnsmasq.d/dnsdock.conf    Restart NetworkManager, either through systemd, or by simply rebooting.  To restart via systemd:  systemctl restart NetworkManager    Run the dnsdock container   docker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm",
            "title": "Method 1: dnsmasq via NetworkManager"
        },
        {
            "location": "/getting-started/linux-installation/#method-2-dnsdock-as-main-resolver",
            "text": "This method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS \nserver. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the \nrest to your normal DNS server.  This example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1.   Run the dnsdock container, specifying your upstream DNS server at the end.   docker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm   Configure 172.17.0.1 as your first DNS resolver in your network configuration. The method for doing this may differ \nbased on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as \nthe first 'nameserver' line in your /etc/resolv.conf file.",
            "title": "Method 2: dnsdock as main resolver"
        },
        {
            "location": "/getting-started/linux-installation/#method-3-libnss-resolver",
            "text": "libnss-resolver is an app that adds Mac-style /etc/resolver/$FQDN files to the Linux NSS resolution stack to query a \ndifferent DNS server for any .vm address.  It may be the easiest option for most installations.  There are releases for Fedora 20, Ubuntu 12.04 and Ubuntu 14.04.   Install  libnss-resolver  Set up .vm hostname resolution  echo 'nameserver 172.17.0.1:53' | sudo tee /etc/resolver/vm    Run the dnsdock container   docker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm",
            "title": "Method 3: libnss-resolver"
        },
        {
            "location": "/getting-started/linux-installation/#running-dnsdock-as-a-service",
            "text": "Create the file  /etc/systemd/system/dnsdock.service  with the following contents   [Unit]\nDescription=DNSDock\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill dnsdock\nExecStartPre=-/usr/bin/docker rm dnsdock\nExecStart=/usr/bin/docker run --rm --name dnsdock -v /var/run/docker.sock:/var/run/docker.sock -l com.dnsdock.name=dnsdock -l com.dnsdock.image=outrigger -p 172.17.0.1:53:53/udp aacebedo/dnsdock:v1.16.1-amd64  --domain=vm\nExecStop=/usr/bin/docker stop dnsdock\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target   Ensure Docker is registered with systemctl  systemctl enable docker    Register dnsdock service:   systemctl enable dnsdock   systemctl daemon-reoload    Now you can start/stop the dnsdock service with   systemctl [start|stop] dnsdock    You can also check its status   systemctl status dnsdock",
            "title": "Running dnsdock as a service"
        },
        {
            "location": "/getting-started/linux-installation/#verifying-dns-is-working",
            "text": "Once you have your environment set up, you can use the following tests to ensure things are running properly.   dig @172.17.0.1 dnsdock.outrigger.vm.  You should get a 172.17.0.0/16 address    ping dnsdock.outrigger.vm  You should get echo replies from a 172.17.0.0/16 address    getent hosts dnsdock.outrigger.vm  You should get a 172.17.0.0/16 address    Verify DNS works between containers  Open a shell for a second test container  docker run --rm -l com.dnsdock.name=test -l com.dnsdock.image=outrigger -it alpine sh    From its prompt:   ping dnsdock.outrigger.vm  You should get echo replies from a 172.17.0.0/16 address",
            "title": "Verifying DNS is working"
        },
        {
            "location": "/getting-started/linux-installation/#troubleshooting",
            "text": "",
            "title": "Troubleshooting"
        },
        {
            "location": "/getting-started/linux-installation/#bind-address-already-in-use",
            "text": "Make sure you are not running a service which binds all mapped IPs.  For example,   docker run -d \\\n  --name=dnsdock \\\n  --restart=always \\\n  -l com.dnsdock.name=dnsdock \\\n  -l com.dnsdock.image=outrigger \\\n  -p 172.17.0.1:53:53/udp \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  aacebedo/dnsdock:v1.16.1-amd64 --domain=vm\n\n  6aa76d0df98ede7e01c1cef53f105a79a60bab72ee905a1acd76ad57d4aeb014\n  docker: Error response from daemon: driver failed programming external connectivity on endpoint dnsdock\n  (62b3788e1f60530d1f468b46833f0bf8227955557da3a135356184f5b7d57bde): Error starting userland proxy: listen\n  udp 172.17.0.1:53: bind: address already in use.  In this case the culprit was  bind9/named  was attaching to all interfaces (on port 53)   The solution  systemctl stop bind9    To avoid having to stop  bind9  every session  systemctl disable bind9    You may need to restart NetworkManager  systemctl restart NetworkManager",
            "title": "bind: address already in use"
        },
        {
            "location": "/getting-started/windows-installation/",
            "text": "Windows Installation\n\n\n\n\nWarning\n\n\nWindows is not yet fully supported but the following information should\nhelp you get a functioning environment running. Submissions of\ninformation that helps improve the command examples and that can be\nrolled into the automation afforded by the rig binary are welcome.\n\n\n\n\nInstall VirtualBox\n\n\nVirtualBox Downloads\n\n\nInstall docker tools\n\n\nYou will need to install the following docker tools. It is recommended\nthat you use \nchocolatey\n or an installation\nmechanism which allows you to simply acquire these binaries with no\nadditional setup.\n\n\n\n\ndocker\n\n\ndocker-machine\n\n\ndocker-compose\n\n\n\n\nDownload the latest \nrig\n binary\n\n\nThe latest Windows binary for rig can be found at https://github.com/phase2/rig/releases.\n\n\nThis binary is responsible for a few tasks on the initial execution of\nthe \nrig start\n command. Information below assumes default values for\nall options.\n\n\n\n\nChecking for the existence of a virtual machine instance to serve as a\ndocker host and creation if it does not exist. Creation is of a virtual\nmachine named dev using a boot2docker iso file based on the version of\nthe \ndocker\n binary installed. This is a command that typically looks like:\n\ndocker-machine create dev --driver=virtualbox --virtualbox-boot2docker-url=https://github.com/boot2docker/boot2docker/releases/download/v17.04.0-ce-rc1/boot2docker.iso --virtualbox-memory=4096 --virtualbox-cpu-count=2 --virtualbox-disk-size=40000 --engine-opt dns=172.17.42.1 --engine-opt bip=172.17.42.1/16 --virtualbox-hostonly-nicpromisc allow-all\n\n\nStartup of created virtual machine serving as docker host.\n\n\nEnsuring proper environmental variable settings for communicating\nwith the running machine. See \ndocker-machine env dev\n for needed\nvariables and values.\n\n\nSetup of routing rules so that all traffic destined for 172.17.0.0/16\nis routed to the IP address of the running instance. This is usually\na command that looks something like:\n\nrunas /noprofile /user:Administrator route DELETE 172.17.0.0\n to clean\nany lingering setup and then\n\nrunas /noprofile /user:Administrator route -p ADD 172.17.0.0/16 $(docker-machine ip dev)\n\n\nSetting up the /data directory within the docker host to point at a\npersistent disk to allow space for non ephemeral data to be stored.\nThere should be a \n/mnt/sda1\n drive available within the virtual machine\nthat a data directory should be created on and then linked to via the\ncommand \nsudo ln -s /mnt/sda1/data /data\n\n\nInstantiation of a dnsdock container to provide DNS services via the\ncommand \ndocker run --restart=always -d -v /var/run/docker.sock:/var/run/docker.sock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=dnsdock --name dnsdock -p 172.17.42.1:53:53/udp phase2/dnsdock\n\n\nSetup of DNS services so that *.vm addresses attempt to resolve\nagainst the running dnsdock container. It is currently unknown how\nto get Windows to accomplish this in a safe way when the containers\nare not running but a work around is to use the \nrig dns-records\n\ncommand to provide output for entry into the\n\nC:\\Windows\\System32\\Drivers\\etc\\hosts\n file.\n\n\nSharing of the home directory of all of your users via NFS with the\ndev instance. On OS X this is accomplished via the \ndocker-machine-nfs.sh\n\nscript. This will allow for code on your local machine to be executed\nfrom within a running container as long as it is in your home directory.",
            "title": "Windows Installation"
        },
        {
            "location": "/getting-started/windows-installation/#windows-installation",
            "text": "Warning  Windows is not yet fully supported but the following information should\nhelp you get a functioning environment running. Submissions of\ninformation that helps improve the command examples and that can be\nrolled into the automation afforded by the rig binary are welcome.",
            "title": "Windows Installation"
        },
        {
            "location": "/getting-started/windows-installation/#install-virtualbox",
            "text": "VirtualBox Downloads",
            "title": "Install VirtualBox"
        },
        {
            "location": "/getting-started/windows-installation/#install-docker-tools",
            "text": "You will need to install the following docker tools. It is recommended\nthat you use  chocolatey  or an installation\nmechanism which allows you to simply acquire these binaries with no\nadditional setup.   docker  docker-machine  docker-compose",
            "title": "Install docker tools"
        },
        {
            "location": "/getting-started/windows-installation/#download-the-latest-rig-binary",
            "text": "The latest Windows binary for rig can be found at https://github.com/phase2/rig/releases.  This binary is responsible for a few tasks on the initial execution of\nthe  rig start  command. Information below assumes default values for\nall options.   Checking for the existence of a virtual machine instance to serve as a\ndocker host and creation if it does not exist. Creation is of a virtual\nmachine named dev using a boot2docker iso file based on the version of\nthe  docker  binary installed. This is a command that typically looks like: docker-machine create dev --driver=virtualbox --virtualbox-boot2docker-url=https://github.com/boot2docker/boot2docker/releases/download/v17.04.0-ce-rc1/boot2docker.iso --virtualbox-memory=4096 --virtualbox-cpu-count=2 --virtualbox-disk-size=40000 --engine-opt dns=172.17.42.1 --engine-opt bip=172.17.42.1/16 --virtualbox-hostonly-nicpromisc allow-all  Startup of created virtual machine serving as docker host.  Ensuring proper environmental variable settings for communicating\nwith the running machine. See  docker-machine env dev  for needed\nvariables and values.  Setup of routing rules so that all traffic destined for 172.17.0.0/16\nis routed to the IP address of the running instance. This is usually\na command that looks something like: runas /noprofile /user:Administrator route DELETE 172.17.0.0  to clean\nany lingering setup and then runas /noprofile /user:Administrator route -p ADD 172.17.0.0/16 $(docker-machine ip dev)  Setting up the /data directory within the docker host to point at a\npersistent disk to allow space for non ephemeral data to be stored.\nThere should be a  /mnt/sda1  drive available within the virtual machine\nthat a data directory should be created on and then linked to via the\ncommand  sudo ln -s /mnt/sda1/data /data  Instantiation of a dnsdock container to provide DNS services via the\ncommand  docker run --restart=always -d -v /var/run/docker.sock:/var/run/docker.sock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=dnsdock --name dnsdock -p 172.17.42.1:53:53/udp phase2/dnsdock  Setup of DNS services so that *.vm addresses attempt to resolve\nagainst the running dnsdock container. It is currently unknown how\nto get Windows to accomplish this in a safe way when the containers\nare not running but a work around is to use the  rig dns-records \ncommand to provide output for entry into the C:\\Windows\\System32\\Drivers\\etc\\hosts  file.  Sharing of the home directory of all of your users via NFS with the\ndev instance. On OS X this is accomplished via the  docker-machine-nfs.sh \nscript. This will allow for code on your local machine to be executed\nfrom within a running container as long as it is in your home directory.",
            "title": "Download the latest rig binary"
        },
        {
            "location": "/getting-started/first-steps/",
            "text": "First Steps\n\n\nTo see a list of the commands that rig provides run \nrig --help\n.\n\nrig\n often offers two levels of commands which allows for grouping all actions related to a similar concept \ntogether. This is most evident with the commands related to a project.\n\n\nTo see detailed help about a particular command, including any available subcommands, run \nrig [command] --help\n.\nFor example, in a project directory, running \nrig project --help\n will show you a list of the \nproject specific actions available to you, as well as other project related subcommands such as \ncreate\n and \nsync\n.\n\n\nTo see detailed help about a particular subcommand, run \nrig [command] [subcommand] --help\n, for example \nrig project create --help\n.",
            "title": "First Steps"
        },
        {
            "location": "/getting-started/first-steps/#first-steps",
            "text": "To see a list of the commands that rig provides run  rig --help . rig  often offers two levels of commands which allows for grouping all actions related to a similar concept \ntogether. This is most evident with the commands related to a project.  To see detailed help about a particular command, including any available subcommands, run  rig [command] --help .\nFor example, in a project directory, running  rig project --help  will show you a list of the \nproject specific actions available to you, as well as other project related subcommands such as  create  and  sync .  To see detailed help about a particular subcommand, run  rig [command] [subcommand] --help , for example  rig project create --help .",
            "title": "First Steps"
        },
        {
            "location": "/project-setup/key-concepts/",
            "text": "Common Setup\n\n\nBefore you begin there are two aspects of Docker and Outrigger setup that you must be aware of.\nThese relate to how you share your code with your running Docker containers and how you ensure that\nany data you need to persist between executions of your containers is preserved. The examples you\nsee throughout this documentation demonstrate configurations that take these aspects into\nconsideration.\n\n\nProject Code\n\n\nThe filesystem within containers is \nephemeral!\n Any changes made there \ndo not persist\n if the \ncontainer is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.)\n\n\nIn typical Docker images the code is built \ndirectly into the container\n at the \n/code\n path. This \nis a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and \nnot need any checkout/file system. You also know when you run a container exactly what code is in \nthere because you generally don\u2019t change the code unless you rebuild the image.  \n\n\nFor development purposes, however, this is problematic because it is burdensome to rebuild an image \nfor each code change. To solve this, there are a few available approaches you can take.\n\n\nApproach 1: NFS Filesystem\n\n\nOutrigger sets up an NFS filesystem from your Host Machine to the Docker host that you can leverage.\nUsing this, you can easily mount your project code from the Host Machine into the running \ncontainer, effectively overriding the files built directly into the image. The running container is \nthen using the local project file system for the overridden paths rather than the file system built \ninto the image.  This allows a developer to use an IDE and edit code directly on the local file system \nof the Host Machine, but execute that code within the environment of the running container.\n\n\n\n\nNote on Project Code Location\n\n\nYour project code \nmust\n be located somewhere within your home directory (\n/Users\n for Macs) on \nyour local machine. This is because NFS shares your home directory into the Docker Host VM, and \nonly files on the Docker Host VM can be referenced in volume shares. \n\n\n\n\nApproach 2: Docker Volumes and Unison\n\n\nThere are some drawbacks to using the NFS Filesystem so your second option is to use a \n\nData Volume\n to hold a mirror of your\ncode base and user Outrigger's \nFilesystem Sync\n capabilities to automatically\nkeep it in sync with your local code base. There are several advantages to this approach detailed\non the filesystem sync page at the cost of a slightly more complex setup. Outrigger helps manage\nthe creation and syncing of the data volume to minimize this complexity.\n\n\nPersistent Data Volume\n\n\nOutrigger maintains a data volume on the Docker Host that is mounted at \n/data\n into every container. \nThis volume is persistent so long as you do not perform a \nrig remove\n operation. This ensures \nthat file access on the VM is done natively for things like databases and other things where filesystem\nperformance matters. If you configure a container to write to this area, you should use a project and \ncontainer based namespace to prevent conflicts as this is a shared resource. For example, you may want \nto use a namespacing method like \n/data/[project name]/[environment]/[service name]\n as a safe location \nto write data. Over time you may find it necessary to manage this directory and delete old project\ndata. This can be done through use of \ndocker-machine ssh dev\n and then direct manipulation of \n/data\n.\n\n\nAny code from your local directories is directly shared in to the Docker Machine VM via NFS at \n/Users\n. \nThis means that even if you destroy and re-create your Docker Host, your code will be safe since it \nlives on the Host Machine.\n\n\n\n\nFILE CHANGES WITHIN A CONTAINER\n\n\nAny files that are generated or changed within a running container that you want to persist after the \ncontainer is stopped \nshould be put onto a persistent volume local machine\n.\n\nA Docker container represents immutable infrastructure, the files on the image \nare able to be changed at runtime but typically do not persist. When the container is stopped and \nrun again it \"boots\" the files that were built into the original image. The volume at \n/data\n (mentioned \nabove) is a persistent volume that you can use and Docker Data Volumes represent another.\n\n\nOutrigger helps maintain the contents of the \n/data\n directory between upgrades of your Docker\nHost Virtual Machine though occasional maintenance to remove old project data will be needed.\nDocker data volumes do not persist between virtual machine upgrades but may be easier to manage\nusing \ndocker volume\n commands.",
            "title": "Key Concepts"
        },
        {
            "location": "/project-setup/key-concepts/#common-setup",
            "text": "Before you begin there are two aspects of Docker and Outrigger setup that you must be aware of.\nThese relate to how you share your code with your running Docker containers and how you ensure that\nany data you need to persist between executions of your containers is preserved. The examples you\nsee throughout this documentation demonstrate configurations that take these aspects into\nconsideration.",
            "title": "Common Setup"
        },
        {
            "location": "/project-setup/key-concepts/#project-code",
            "text": "The filesystem within containers is  ephemeral!  Any changes made there  do not persist  if the \ncontainer is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.)  In typical Docker images the code is built  directly into the container  at the  /code  path. This \nis a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and \nnot need any checkout/file system. You also know when you run a container exactly what code is in \nthere because you generally don\u2019t change the code unless you rebuild the image.    For development purposes, however, this is problematic because it is burdensome to rebuild an image \nfor each code change. To solve this, there are a few available approaches you can take.",
            "title": "Project Code"
        },
        {
            "location": "/project-setup/key-concepts/#approach-1-nfs-filesystem",
            "text": "Outrigger sets up an NFS filesystem from your Host Machine to the Docker host that you can leverage.\nUsing this, you can easily mount your project code from the Host Machine into the running \ncontainer, effectively overriding the files built directly into the image. The running container is \nthen using the local project file system for the overridden paths rather than the file system built \ninto the image.  This allows a developer to use an IDE and edit code directly on the local file system \nof the Host Machine, but execute that code within the environment of the running container.   Note on Project Code Location  Your project code  must  be located somewhere within your home directory ( /Users  for Macs) on \nyour local machine. This is because NFS shares your home directory into the Docker Host VM, and \nonly files on the Docker Host VM can be referenced in volume shares.",
            "title": "Approach 1: NFS Filesystem"
        },
        {
            "location": "/project-setup/key-concepts/#approach-2-docker-volumes-and-unison",
            "text": "There are some drawbacks to using the NFS Filesystem so your second option is to use a  Data Volume  to hold a mirror of your\ncode base and user Outrigger's  Filesystem Sync  capabilities to automatically\nkeep it in sync with your local code base. There are several advantages to this approach detailed\non the filesystem sync page at the cost of a slightly more complex setup. Outrigger helps manage\nthe creation and syncing of the data volume to minimize this complexity.",
            "title": "Approach 2: Docker Volumes and Unison"
        },
        {
            "location": "/project-setup/key-concepts/#persistent-data-volume",
            "text": "Outrigger maintains a data volume on the Docker Host that is mounted at  /data  into every container. \nThis volume is persistent so long as you do not perform a  rig remove  operation. This ensures \nthat file access on the VM is done natively for things like databases and other things where filesystem\nperformance matters. If you configure a container to write to this area, you should use a project and \ncontainer based namespace to prevent conflicts as this is a shared resource. For example, you may want \nto use a namespacing method like  /data/[project name]/[environment]/[service name]  as a safe location \nto write data. Over time you may find it necessary to manage this directory and delete old project\ndata. This can be done through use of  docker-machine ssh dev  and then direct manipulation of  /data .  Any code from your local directories is directly shared in to the Docker Machine VM via NFS at  /Users . \nThis means that even if you destroy and re-create your Docker Host, your code will be safe since it \nlives on the Host Machine.   FILE CHANGES WITHIN A CONTAINER  Any files that are generated or changed within a running container that you want to persist after the \ncontainer is stopped  should be put onto a persistent volume local machine . \nA Docker container represents immutable infrastructure, the files on the image \nare able to be changed at runtime but typically do not persist. When the container is stopped and \nrun again it \"boots\" the files that were built into the original image. The volume at  /data  (mentioned \nabove) is a persistent volume that you can use and Docker Data Volumes represent another.  Outrigger helps maintain the contents of the  /data  directory between upgrades of your Docker\nHost Virtual Machine though occasional maintenance to remove old project data will be needed.\nDocker data volumes do not persist between virtual machine upgrades but may be easier to manage\nusing  docker volume  commands.",
            "title": "Persistent Data Volume"
        },
        {
            "location": "/project-setup/new-projects/",
            "text": "New Projects\n\n\nTo setup new projects in Outrigger use the command \nrig project create\n to coordinate all of the various operations \nrequired to get the proper configuration in place. The \nproject create\n command runs a collection of Yeoman generators, \nprompting for input and outputting a collection of Docker Compose and Outrigger configuration.\n\n\nProject Create command\n\n\nThe \nproject create\n command has the following usage:\n\n\nrig project create [--image image:tag] [type] [args]\n\n\n\n\nRunning the \nproject create\n command without specifying the \n--image\n flag will run using the \nOutrigger Generator image\n.\nAdditionally, if no \n[type]\n is specified the default \ntype\n is \noutrigger-drupal\n.  Other options for \n[type]\n on the default\n\noutrigger/generator\n image are \ngadget\n and \npattern-lab-starter\n.\n\n\nDocumentation for the currently supported types and related args can be found here:\n\n\n\n\noutrigger-drupal\n\n\ngadget\n\n\npatern-lab-starter\n\n\n\n\n\n\nSpecify a Generator Image\n\n\nIf you want to use a different image other than \noutrigger/generator:latest\n you can specify it with the \n--image\n flag.\nGenerator images should expect to output their work at path \n/generated\n within the running container.\nThe \nproject create\n command mounts the current working directory at this path in order to capture generated\noutput to your host machine. All \n[type]\n and \n[arg]\n options are passed to the \ndocker run\n command\nlaunching the container for the specified image.",
            "title": "New Projects"
        },
        {
            "location": "/project-setup/new-projects/#new-projects",
            "text": "To setup new projects in Outrigger use the command  rig project create  to coordinate all of the various operations \nrequired to get the proper configuration in place. The  project create  command runs a collection of Yeoman generators, \nprompting for input and outputting a collection of Docker Compose and Outrigger configuration.",
            "title": "New Projects"
        },
        {
            "location": "/project-setup/new-projects/#project-create-command",
            "text": "The  project create  command has the following usage:  rig project create [--image image:tag] [type] [args]  Running the  project create  command without specifying the  --image  flag will run using the  Outrigger Generator image .\nAdditionally, if no  [type]  is specified the default  type  is  outrigger-drupal .  Other options for  [type]  on the default outrigger/generator  image are  gadget  and  pattern-lab-starter .  Documentation for the currently supported types and related args can be found here:   outrigger-drupal  gadget  patern-lab-starter    Specify a Generator Image  If you want to use a different image other than  outrigger/generator:latest  you can specify it with the  --image  flag.\nGenerator images should expect to output their work at path  /generated  within the running container.\nThe  project create  command mounts the current working directory at this path in order to capture generated\noutput to your host machine. All  [type]  and  [arg]  options are passed to the  docker run  command\nlaunching the container for the specified image.",
            "title": "Project Create command"
        },
        {
            "location": "/project-setup/existing-projects/",
            "text": "Existing Projects\n\n\nTo add Outrigger support to an existing project, you'll need to understand how to create:\n\n\n\n\nDocker Compose file(s)\n\n\nCustom Domain Names for Project Services \n\n\nLocal code sharing with your containers\n\n\nPersistent data storage\n\n\nAn (\noptional\n) \n.outrigger.yml\n file\n\n\n\n\nDocker Compose file(s)\n\n\nOutrigger is intended to work with either raw Docker commands or Docker Compose files, and we find things are best when\norchestrating your local environments using Docker Compose.\n\n\nBe sure to read the \nDocker Compose compose file documentation\n\nso that you have a handle on how to setup the needed configuration.\n\n\nDefine Project Services\n\n\nThe essential point is that you will need to define a Docker Compose Service for each of the components in your architecture.\nWeb/API Server, database, cache/key-value store, search server, proxy cache, mail server, etc. Each of those will be configured\nas a service in your \ndocker-compose.yml\n file.\n\n\nPlease see the \nOutrigger Examples Repository\n to see setups of various \ntechnologies within Outrigger.  Specifically, the \nDrupal 8 example\n \ncovers all of the high points.\n\n\nBuild Container\n\n\nIn addition to containers for all of your project services, we find it useful to have a container we refer to as the \n\nbuild\n container. This container provides the tools supporting development of your project. \n\n\nThe default build image for Outrigger is \noutrigger/build\n. It includes tools supporting a wide variety of modern\nweb development such as \nnode\n, \nnpm\n, \ncomposer\n, \ngrunt\n, \ngulp\n, \nbower\n, etc.\n\n\nWe often configure the build container and associated command containers in a file named \nbuild.yml\n. The Drupal 8\nexample mentioned before contains a sample a \nbuild.yml\n\nfile.\n\n\nChoosing Images For Project Services\n\n\nAny valid Docker Image can work with Outrigger. We also supply \na collection of crafted Docker\nimages\n that are easily configured and optimized. An example for\nan Apache / PHP image based on PHP7 is \noutrigger/apache-php:php70\n or \nphp:7.1.1-apache\n  \n\n\n\n\nUse Image Tags\n\n\nWhen selecting and image to use, be sure to also specify a tag for that image. When you do not\nspecify a tag, the default tag of \nlatest\n is used and it is easy to get out of sync with other\nteam members or simply get unwanted changes if you do not explicitly specify an image tag.\n\n\n\n\nGeneral Config\n\n\nOnce you set up your \ndocker-compose.yml\n file, you'll likely need to customize your services for a variety of reasons. \nMany images will use environment variables and/or volume mounts to customize the running container configuration. See\n\nCustomizing Container Configuration\n to learn more about \ncustomizing your configuration for your project. \n\n\nCustom Domain Names for Project Services\n\n\nSee \nDNS Resolution\n to learn how to use Docker Labels to specify\ncustom domain names for your project's services.\n\n\nSharing code into and across containers\n\n\nThe straightforward way to get code from your host into containers is to use an NFS volume mount.  This allows you, for\nexample, to mount the current directory (\n.\n) to a specific place inside a container (\n/var/www\n) with a simple volume \nmount specification in your \ndocker-compose.yml\n file. See the \nVolume Mount\n \ndocumentation on volume mount specifications. For example:\n\n\n.:/var/www\n\n\n\nIf filesystem performance for builds or filesystem notifications for watches are what you need, check out the \n\nFilesystem Sync\n documentation to guide you through setup.\n\n\nPersistent Data Storage\n\n\nAnd finally we have a Persistent Data Volume, \n/data\n, that lives within the Docker Machine. Be sure to read about the \nPersistent Data Volume (\n/data\n) in the \nKey Concepts\n documentation. Often data may need to persist across \ncontainer runs while needing some level of performance but not need to be shared directly with the host computer. \nThings like the \n/var/lib/mysql\n data directory are often in this category as you may want to export a DB to your host \ncomputer, but you very rarely want the raw MySQL data files.  For these cases a volume mount from \n/data\n into your containers\nis what you need.  For example:\n\n\n/data/project-one/local/mysql:/var/lib/mysql\n\n\n\n.outrigger.yml\n\n\nFinally, you can customize the \nrig project run\n command with common or project specific scripts to provide a considerable\nupgrade to the developer experience. See \nProject Configuration\n for details on how to use\nan \n.outrigger.yml\n file in your project directory to have project specific configuration integrated into \nrig\n.",
            "title": "Existing Projects"
        },
        {
            "location": "/project-setup/existing-projects/#existing-projects",
            "text": "To add Outrigger support to an existing project, you'll need to understand how to create:   Docker Compose file(s)  Custom Domain Names for Project Services   Local code sharing with your containers  Persistent data storage  An ( optional )  .outrigger.yml  file",
            "title": "Existing Projects"
        },
        {
            "location": "/project-setup/existing-projects/#docker-compose-files",
            "text": "Outrigger is intended to work with either raw Docker commands or Docker Compose files, and we find things are best when\norchestrating your local environments using Docker Compose.  Be sure to read the  Docker Compose compose file documentation \nso that you have a handle on how to setup the needed configuration.",
            "title": "Docker Compose file(s)"
        },
        {
            "location": "/project-setup/existing-projects/#define-project-services",
            "text": "The essential point is that you will need to define a Docker Compose Service for each of the components in your architecture.\nWeb/API Server, database, cache/key-value store, search server, proxy cache, mail server, etc. Each of those will be configured\nas a service in your  docker-compose.yml  file.  Please see the  Outrigger Examples Repository  to see setups of various \ntechnologies within Outrigger.  Specifically, the  Drupal 8 example  \ncovers all of the high points.",
            "title": "Define Project Services"
        },
        {
            "location": "/project-setup/existing-projects/#build-container",
            "text": "In addition to containers for all of your project services, we find it useful to have a container we refer to as the  build  container. This container provides the tools supporting development of your project.   The default build image for Outrigger is  outrigger/build . It includes tools supporting a wide variety of modern\nweb development such as  node ,  npm ,  composer ,  grunt ,  gulp ,  bower , etc.  We often configure the build container and associated command containers in a file named  build.yml . The Drupal 8\nexample mentioned before contains a sample a  build.yml \nfile.",
            "title": "Build Container"
        },
        {
            "location": "/project-setup/existing-projects/#choosing-images-for-project-services",
            "text": "Any valid Docker Image can work with Outrigger. We also supply  a collection of crafted Docker\nimages  that are easily configured and optimized. An example for\nan Apache / PHP image based on PHP7 is  outrigger/apache-php:php70  or  php:7.1.1-apache      Use Image Tags  When selecting and image to use, be sure to also specify a tag for that image. When you do not\nspecify a tag, the default tag of  latest  is used and it is easy to get out of sync with other\nteam members or simply get unwanted changes if you do not explicitly specify an image tag.",
            "title": "Choosing Images For Project Services"
        },
        {
            "location": "/project-setup/existing-projects/#general-config",
            "text": "Once you set up your  docker-compose.yml  file, you'll likely need to customize your services for a variety of reasons. \nMany images will use environment variables and/or volume mounts to customize the running container configuration. See Customizing Container Configuration  to learn more about \ncustomizing your configuration for your project.",
            "title": "General Config"
        },
        {
            "location": "/project-setup/existing-projects/#custom-domain-names-for-project-services",
            "text": "See  DNS Resolution  to learn how to use Docker Labels to specify\ncustom domain names for your project's services.",
            "title": "Custom Domain Names for Project Services"
        },
        {
            "location": "/project-setup/existing-projects/#sharing-code-into-and-across-containers",
            "text": "The straightforward way to get code from your host into containers is to use an NFS volume mount.  This allows you, for\nexample, to mount the current directory ( . ) to a specific place inside a container ( /var/www ) with a simple volume \nmount specification in your  docker-compose.yml  file. See the  Volume Mount  \ndocumentation on volume mount specifications. For example:  .:/var/www  If filesystem performance for builds or filesystem notifications for watches are what you need, check out the  Filesystem Sync  documentation to guide you through setup.",
            "title": "Sharing code into and across containers"
        },
        {
            "location": "/project-setup/existing-projects/#persistent-data-storage",
            "text": "And finally we have a Persistent Data Volume,  /data , that lives within the Docker Machine. Be sure to read about the \nPersistent Data Volume ( /data ) in the  Key Concepts  documentation. Often data may need to persist across \ncontainer runs while needing some level of performance but not need to be shared directly with the host computer. \nThings like the  /var/lib/mysql  data directory are often in this category as you may want to export a DB to your host \ncomputer, but you very rarely want the raw MySQL data files.  For these cases a volume mount from  /data  into your containers\nis what you need.  For example:  /data/project-one/local/mysql:/var/lib/mysql",
            "title": "Persistent Data Storage"
        },
        {
            "location": "/project-setup/existing-projects/#outriggeryml",
            "text": "Finally, you can customize the  rig project run  command with common or project specific scripts to provide a considerable\nupgrade to the developer experience. See  Project Configuration  for details on how to use\nan  .outrigger.yml  file in your project directory to have project specific configuration integrated into  rig .",
            "title": ".outrigger.yml"
        },
        {
            "location": "/project-setup/project-configuration/",
            "text": "Project Configuration\n\n\nProjects can have a local \n.outrigger.yml\n file included in their project to define the \nrig project\n commands. This can\nbe done to streamline the developer experience of the project, or to capture project specifics / intricacies in a place that\nis consistent for all team members.\n\n\nFile Location\n\n\nThe configuration file is typically named \n.outrigger.yml\n and is normally placed at the root of a project directory tree.\n\n\nYou may specify a $RIG_PROJECT_CONFIG_FILE environment variable to override this which can be useful for running rig project\ncommands from different directories.\n\n\nFeatures\n\n\nProject Scripts\n\n\nMuch like composer and npm, rig project supports the creation of project-standardized scripts in this configuration file.\n\n\n\n\nScripts are executed relative to the location of the configuration file.\n\n\nThe \nbin\n property is prepended to your $PATH variable, simplifying the script configuration.\n\n\nEach \"script\" may be made up of a series of steps, if any step fails remaining steps will not be executed.\n\n\nWhen you run a script (such as \nrig project run:up\n) you can include additional parameters.\n  Such additonal parameters will be appended to the final step of the script.\n\n\n\n\nCommands are defined in rig project based on the script ID prefixed by \nrun:\n, so a command such as\n\nup\n is primarily available as \nrig project run:up\n.\n\n\nAny script may specify a single alias to shorten this. An alias such as 'up' would change this to \nrig project up\n.\n\n\nConfigure Filesystem Sync\n\n\nAs described in \nFilesystem Sync\n, rig project brokers the synchronization process.\n\n\nYou can tailor this process to specify the name of the Docker volume or set files or directories to be\nskipped by this process as a performance optimization.\n\n\nNone of this configuration is required.\n\n\nConfiguration File\n\n\nBelow is a sample configuration file with comments to describe the various configuration options.\n\n\n# Required version so we can ensure compatibility\nversion: 1.0\n\n# This is prepended to the $PATH for any commands referenced in the scripts section.\nbin: ./bin:./node_modules/.bin:./vendor/bin\n\n# Project Scripts\n# These can be run via 'rig project run:\nkey\n'\n# If you specify an alias, you can run 'rig project \nalias\n'\nscripts:\n\n  # This will be `rig project run:up`\n  up:\n    # Or with an alias, `rig project up`\n    alias: up\n    # Description is used for help when running `rig project help` or `rig project run help`\n    description: Start up operational docker containers and filesystem sync.\n    # These are the various run steps, they will be concatenated together into a single command with '\n'\n    run:\n      - rig project sync:start\n      - docker-compose up -d\n\n  stop:\n    alias: down\n    description: Halt operational containers and filesystem sync.\n    run:\n      - docker-compose stop\n      - rig project sync:stop\n\n# This controls configuration for the `project sync:start` command.\nsync:\n  # This is the name of the external volume to use (optional). Needs to match volume name in Docker Compose.\n  volume: project-sync\n  # These ignores will be added to unison and not synced between local and project volume\n  ignores:\n    - \nName .git\n\n    - \nPath build/tmp\n\n    - \nRegex full/path/to/*.json\n\n\n\n\n\n\nSee \nthis Outrigger Project configuration file\n,\nfrom the \noutrigger-drupal\n generator, for an example of a detailed \n.outrigger.yml\n file.",
            "title": "Project Configuration"
        },
        {
            "location": "/project-setup/project-configuration/#project-configuration",
            "text": "Projects can have a local  .outrigger.yml  file included in their project to define the  rig project  commands. This can\nbe done to streamline the developer experience of the project, or to capture project specifics / intricacies in a place that\nis consistent for all team members.",
            "title": "Project Configuration"
        },
        {
            "location": "/project-setup/project-configuration/#file-location",
            "text": "The configuration file is typically named  .outrigger.yml  and is normally placed at the root of a project directory tree.  You may specify a $RIG_PROJECT_CONFIG_FILE environment variable to override this which can be useful for running rig project\ncommands from different directories.",
            "title": "File Location"
        },
        {
            "location": "/project-setup/project-configuration/#features",
            "text": "",
            "title": "Features"
        },
        {
            "location": "/project-setup/project-configuration/#project-scripts",
            "text": "Much like composer and npm, rig project supports the creation of project-standardized scripts in this configuration file.   Scripts are executed relative to the location of the configuration file.  The  bin  property is prepended to your $PATH variable, simplifying the script configuration.  Each \"script\" may be made up of a series of steps, if any step fails remaining steps will not be executed.  When you run a script (such as  rig project run:up ) you can include additional parameters.\n  Such additonal parameters will be appended to the final step of the script.   Commands are defined in rig project based on the script ID prefixed by  run: , so a command such as up  is primarily available as  rig project run:up .  Any script may specify a single alias to shorten this. An alias such as 'up' would change this to  rig project up .",
            "title": "Project Scripts"
        },
        {
            "location": "/project-setup/project-configuration/#configure-filesystem-sync",
            "text": "As described in  Filesystem Sync , rig project brokers the synchronization process.  You can tailor this process to specify the name of the Docker volume or set files or directories to be\nskipped by this process as a performance optimization.  None of this configuration is required.",
            "title": "Configure Filesystem Sync"
        },
        {
            "location": "/project-setup/project-configuration/#configuration-file",
            "text": "Below is a sample configuration file with comments to describe the various configuration options.  # Required version so we can ensure compatibility\nversion: 1.0\n\n# This is prepended to the $PATH for any commands referenced in the scripts section.\nbin: ./bin:./node_modules/.bin:./vendor/bin\n\n# Project Scripts\n# These can be run via 'rig project run: key '\n# If you specify an alias, you can run 'rig project  alias '\nscripts:\n\n  # This will be `rig project run:up`\n  up:\n    # Or with an alias, `rig project up`\n    alias: up\n    # Description is used for help when running `rig project help` or `rig project run help`\n    description: Start up operational docker containers and filesystem sync.\n    # These are the various run steps, they will be concatenated together into a single command with ' '\n    run:\n      - rig project sync:start\n      - docker-compose up -d\n\n  stop:\n    alias: down\n    description: Halt operational containers and filesystem sync.\n    run:\n      - docker-compose stop\n      - rig project sync:stop\n\n# This controls configuration for the `project sync:start` command.\nsync:\n  # This is the name of the external volume to use (optional). Needs to match volume name in Docker Compose.\n  volume: project-sync\n  # These ignores will be added to unison and not synced between local and project volume\n  ignores:\n    -  Name .git \n    -  Path build/tmp \n    -  Regex full/path/to/*.json   See  this Outrigger Project configuration file ,\nfrom the  outrigger-drupal  generator, for an example of a detailed  .outrigger.yml  file.",
            "title": "Configuration File"
        },
        {
            "location": "/project-setup/filesystem-sync/",
            "text": "Filesystem Sync\n\n\nThe default NFS filesystems that Outrigger sets up provides easy sharing of code and files with your project containers. \nHowever, NFS can be slow when writing, reading, or scanning thousands of files when compared to native filesystem performance. \nSome modern tool kits and package managers favor large numbers of small files and libraries so maintaining high performance \nof a build becomes challenging when your code base is on a volume mounted NFS share.\n\n\nNFS also has the downside of not propagating filesystem modification events. This can be problematic if you want to run\na process in your container to rebuild or update compiled project assets when modification notifications are triggered\ndue to local file editing in your IDE.\n\n\nIn order to better support these types of operations, Outrigger provides support for multi-directional filesystem syncing \n(which includes filesystem notifications) using \nunison\n.  Your project root folder is synced via \nunison\n into a sync \ncontainer and exposed as a Docker Volume to the rest of your application. This volume is then used as your mount point \ninstead of the NFS share and allows nearly native filesystem performance and features.\n\n\nSync Volume Name\n\n\nThe name of the sync volume can be set in a variety of ways. It is determined using the following precedence:\n\n\n\n\nArgument provided to the \nproject sync:start\n command\n\n\nSpecified in the \nsync\n -\n \nvolume-name\n property of your \nOutrigger Project Configuration\n\n\nIn your \ndocker-compose.yml\n file, specified as an \nexternal\n volume with a name of the pattern \n*-sync\n\n\nIf not specified anywhere else, it will use the name of the current project folder with \n-sync\n appended\n\n\n\n\nWe recommend using approach #3, as it is the most explicit and is in line with all of the configuration you will need\nto take full advantage of \nunison\n syncing.\n\n\nSetting up sync for your containers\n\n\nTo setup \nunison\n sync for your containers you will need to \nreference an external volume\n \nin your \ndocker-compose.yml\n and use that external volume in the mount specification for any services that need to \nreference the same set of files to ensure all containers stay in sync. Also specifying the sync volume in your \nbuild.yml\n\nfile for any containers that reference your code base will have a considerable performance improvement.\n\n\nAdd the volume spec to your compose file\n\n\nThis needs to be placed at the root level. It cannot be a child of \nservices\n.\n\n\nvolumes:\n  project-sync:\n    external: true\n\n\n\n\n\n\nNote about volume name\n\n\nIf you optionally specified a volume name to the \nproject sync:start\n command or configured a name in your Outrigger \nProject Configuration file the name in this volumes section must match the one specified to the \nproject sync:start\n\ncommand. The reason why we don't exclusively grab the volume name from the compose file is that this volume may be \nmanaged outside of Outrigger by another tool and we want to support named volumes that don't precisely follow \nthe \n*-sync\n convention we lay out here. \n\n\n\n\nUse the external volume as your local mount point\n\n\nservices:\n  build:\n    image: outrigger/build:php71\n    volumes:\n      - project-sync:/var/www\n\n\n\n\nStart it up\n\n\nFrom your project root run the command \nrig project sync:start\n (there is also an alias \nrig project sync\n) to get going.\n\nWith the above configuration you should be up and running. The initial sync can take a few seconds depending on the size\nof your project folder. You will see a progress indicator that will let you know when the initial sync is finished and \nthings are ready to use.\n\n\nCleaning up\n\n\nSince the \nproject sync:start\n command starts another container to manage the volume syncing, we have a command \nproject sync:stop\n\nthat will clean up that running container for you. It discovers the volume / container name the same way \nproject sync:start\n \ndoes.\n\n\nWhen you are done for the day, or for that project, from the project root \nrig project sync:stop\n will clean up any running\n\nunison\n containers for that project.",
            "title": "Filesystem Sync"
        },
        {
            "location": "/project-setup/filesystem-sync/#filesystem-sync",
            "text": "The default NFS filesystems that Outrigger sets up provides easy sharing of code and files with your project containers. \nHowever, NFS can be slow when writing, reading, or scanning thousands of files when compared to native filesystem performance. \nSome modern tool kits and package managers favor large numbers of small files and libraries so maintaining high performance \nof a build becomes challenging when your code base is on a volume mounted NFS share.  NFS also has the downside of not propagating filesystem modification events. This can be problematic if you want to run\na process in your container to rebuild or update compiled project assets when modification notifications are triggered\ndue to local file editing in your IDE.  In order to better support these types of operations, Outrigger provides support for multi-directional filesystem syncing \n(which includes filesystem notifications) using  unison .  Your project root folder is synced via  unison  into a sync \ncontainer and exposed as a Docker Volume to the rest of your application. This volume is then used as your mount point \ninstead of the NFS share and allows nearly native filesystem performance and features.",
            "title": "Filesystem Sync"
        },
        {
            "location": "/project-setup/filesystem-sync/#sync-volume-name",
            "text": "The name of the sync volume can be set in a variety of ways. It is determined using the following precedence:   Argument provided to the  project sync:start  command  Specified in the  sync  -   volume-name  property of your  Outrigger Project Configuration  In your  docker-compose.yml  file, specified as an  external  volume with a name of the pattern  *-sync  If not specified anywhere else, it will use the name of the current project folder with  -sync  appended   We recommend using approach #3, as it is the most explicit and is in line with all of the configuration you will need\nto take full advantage of  unison  syncing.",
            "title": "Sync Volume Name"
        },
        {
            "location": "/project-setup/filesystem-sync/#setting-up-sync-for-your-containers",
            "text": "To setup  unison  sync for your containers you will need to  reference an external volume  \nin your  docker-compose.yml  and use that external volume in the mount specification for any services that need to \nreference the same set of files to ensure all containers stay in sync. Also specifying the sync volume in your  build.yml \nfile for any containers that reference your code base will have a considerable performance improvement.  Add the volume spec to your compose file  This needs to be placed at the root level. It cannot be a child of  services .  volumes:\n  project-sync:\n    external: true   Note about volume name  If you optionally specified a volume name to the  project sync:start  command or configured a name in your Outrigger \nProject Configuration file the name in this volumes section must match the one specified to the  project sync:start \ncommand. The reason why we don't exclusively grab the volume name from the compose file is that this volume may be \nmanaged outside of Outrigger by another tool and we want to support named volumes that don't precisely follow \nthe  *-sync  convention we lay out here.    Use the external volume as your local mount point  services:\n  build:\n    image: outrigger/build:php71\n    volumes:\n      - project-sync:/var/www",
            "title": "Setting up sync for your containers"
        },
        {
            "location": "/project-setup/filesystem-sync/#start-it-up",
            "text": "From your project root run the command  rig project sync:start  (there is also an alias  rig project sync ) to get going. \nWith the above configuration you should be up and running. The initial sync can take a few seconds depending on the size\nof your project folder. You will see a progress indicator that will let you know when the initial sync is finished and \nthings are ready to use.",
            "title": "Start it up"
        },
        {
            "location": "/project-setup/filesystem-sync/#cleaning-up",
            "text": "Since the  project sync:start  command starts another container to manage the volume syncing, we have a command  project sync:stop \nthat will clean up that running container for you. It discovers the volume / container name the same way  project sync:start  \ndoes.  When you are done for the day, or for that project, from the project root  rig project sync:stop  will clean up any running unison  containers for that project.",
            "title": "Cleaning up"
        },
        {
            "location": "/project-setup/docker-images/",
            "text": "Docker Images\n\n\nKey components\n\n\nThe following Docker Images have been built to work with Outrigger. They all have a similar and consistent \nsetup, so when using these images it is important to know the technology in place and how to customize it for you purposes.\n\nWe have provided environmental configuration for the most frequently customized options, and any extended customization \ncan be made by following the recommendations in \nCustomizing container configuration\n\n\nThese images are built with the following software\n\n\n\n\nS6-overlay\n Init System\n\n\nconfd\n config file templating\n\n\n\n\n\n\nNote\n\n\nBelow are a collection of tuned images for working with Outrigger, but \nany\n Docker Image can be used within Outrigger.\nAdditionally these Docker Images do not need to be used with Outrigger, they can be used in any Docker environment.\n\n\n\n\nImages\n\n\noutrigger/servicebase\n\n\n(\nDocker Hub\n) (\nRepo\n) \n\n\nA CentOS 7 base image that has s6-overlay and confd. This is a useful image for extending to build non-trivial services.\n\n\noutrigger/servicebaselight\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nThis is an Alpine-based image that has had the S6-overlay init system and confd added to it.\n\n\nIn addition to the lightweight Alpine base it also includes bash and glibc so that Go-based Linux binaries will run. This \nimage is only ~8MB when compared to the 100+MB of servicebase.\n\n\noutrigger/apache-php\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nApache \n PHP runtime images. It currently support PHP 5.5, 5.6, and 7.0\n\n\noutrigger/apache-php-base\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nA base image for outrigger/apache-php. Includes Apache and a default VirtualHost configured with a proxy to PHP-FPM. It \ndoes not include the php runtime, that is handed in the extension image(s).\n\n\noutrigger/build\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nThis image provides the collection of development tools necessary to build applications, bundled with a wide array of \ntools useful for development and troubleshooting via the command-line interface. While it is possible to directly\nconnect via the \"web\" containers (apache-php), this is the preferred way to perform \"server work\".\n\n\nIt also contains some extras you may need to work with Drupal, including use of tools such as \n\nDrupal Console\n, \nGrunt Drupal Tasks\n and \n\nPattern Lab Starter\n.\n\n\noutrigger/mariadb\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nMariaDB image for MySQL based builds with confd templates for config\n\n\noutrigger/memcache\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nMemcache image with configurable settings\n\n\noutrigger/redis\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nRedis image with a confd template for redis.conf\n\n\noutrigger/node\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nNode image \n\n\noutrigger/varnish\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nVarnish container with fancy environment variables for easy configuration\n\n\noutrigger/jenkins-docker\n\n\n(\nDocker Hub\n) (\nRepo\n)\n\n\nJenkins image that is built to be able to run Docker commands and launch containers. Docker-inception.",
            "title": "Docker Images"
        },
        {
            "location": "/project-setup/docker-images/#docker-images",
            "text": "",
            "title": "Docker Images"
        },
        {
            "location": "/project-setup/docker-images/#key-components",
            "text": "The following Docker Images have been built to work with Outrigger. They all have a similar and consistent \nsetup, so when using these images it is important to know the technology in place and how to customize it for you purposes. \nWe have provided environmental configuration for the most frequently customized options, and any extended customization \ncan be made by following the recommendations in  Customizing container configuration  These images are built with the following software   S6-overlay  Init System  confd  config file templating    Note  Below are a collection of tuned images for working with Outrigger, but  any  Docker Image can be used within Outrigger.\nAdditionally these Docker Images do not need to be used with Outrigger, they can be used in any Docker environment.",
            "title": "Key components"
        },
        {
            "location": "/project-setup/docker-images/#images",
            "text": "",
            "title": "Images"
        },
        {
            "location": "/project-setup/docker-images/#outriggerservicebase",
            "text": "( Docker Hub ) ( Repo )   A CentOS 7 base image that has s6-overlay and confd. This is a useful image for extending to build non-trivial services.",
            "title": "outrigger/servicebase"
        },
        {
            "location": "/project-setup/docker-images/#outriggerservicebaselight",
            "text": "( Docker Hub ) ( Repo )  This is an Alpine-based image that has had the S6-overlay init system and confd added to it.  In addition to the lightweight Alpine base it also includes bash and glibc so that Go-based Linux binaries will run. This \nimage is only ~8MB when compared to the 100+MB of servicebase.",
            "title": "outrigger/servicebaselight"
        },
        {
            "location": "/project-setup/docker-images/#outriggerapache-php",
            "text": "( Docker Hub ) ( Repo )  Apache   PHP runtime images. It currently support PHP 5.5, 5.6, and 7.0",
            "title": "outrigger/apache-php"
        },
        {
            "location": "/project-setup/docker-images/#outriggerapache-php-base",
            "text": "( Docker Hub ) ( Repo )  A base image for outrigger/apache-php. Includes Apache and a default VirtualHost configured with a proxy to PHP-FPM. It \ndoes not include the php runtime, that is handed in the extension image(s).",
            "title": "outrigger/apache-php-base"
        },
        {
            "location": "/project-setup/docker-images/#outriggerbuild",
            "text": "( Docker Hub ) ( Repo )  This image provides the collection of development tools necessary to build applications, bundled with a wide array of \ntools useful for development and troubleshooting via the command-line interface. While it is possible to directly\nconnect via the \"web\" containers (apache-php), this is the preferred way to perform \"server work\".  It also contains some extras you may need to work with Drupal, including use of tools such as  Drupal Console ,  Grunt Drupal Tasks  and  Pattern Lab Starter .",
            "title": "outrigger/build"
        },
        {
            "location": "/project-setup/docker-images/#outriggermariadb",
            "text": "( Docker Hub ) ( Repo )  MariaDB image for MySQL based builds with confd templates for config",
            "title": "outrigger/mariadb"
        },
        {
            "location": "/project-setup/docker-images/#outriggermemcache",
            "text": "( Docker Hub ) ( Repo )  Memcache image with configurable settings",
            "title": "outrigger/memcache"
        },
        {
            "location": "/project-setup/docker-images/#outriggerredis",
            "text": "( Docker Hub ) ( Repo )  Redis image with a confd template for redis.conf",
            "title": "outrigger/redis"
        },
        {
            "location": "/project-setup/docker-images/#outriggernode",
            "text": "( Docker Hub ) ( Repo )  Node image",
            "title": "outrigger/node"
        },
        {
            "location": "/project-setup/docker-images/#outriggervarnish",
            "text": "( Docker Hub ) ( Repo )  Varnish container with fancy environment variables for easy configuration",
            "title": "outrigger/varnish"
        },
        {
            "location": "/project-setup/docker-images/#outriggerjenkins-docker",
            "text": "( Docker Hub ) ( Repo )  Jenkins image that is built to be able to run Docker commands and launch containers. Docker-inception.",
            "title": "outrigger/jenkins-docker"
        },
        {
            "location": "/common-tasks/starting-your-project-containers/",
            "text": "Starting your project containers\n\n\nConfigure your environment\n\n\nEnsure all terminals you intend to use can communicate with the Docker Host.  Generally a simple \ndocker ps\n will either\nlist out running containers or give you an error like \nCannot connect to the Docker daemon. Is the docker daemon running \non this host?\n\n\nIf you get the error make sure you have run \neval \"$(rig config)\"\n. See \n\nInstallation\n for the proper way to configure your environments.\n\n\nStart your containers\n\n\nIn the project directory, start the containers with: \ndocker-compose up\n\n\n\n\nNote\n\n\nThe docker-compose command runs in the foreground as long as the Docker containers are running. \n(It is not hung \nif there is no output after \"Attaching [container name]...)\n\n\n\n\nLogs from the running containers will stream to the console and be prefixed by their compose names plus an \ninteger (i.e. web_1, db_1, etc)\n\n\nYou can start another terminal tab if you need to run other commands (such as the build container operations, etc.)",
            "title": "Starting your project containers"
        },
        {
            "location": "/common-tasks/starting-your-project-containers/#starting-your-project-containers",
            "text": "",
            "title": "Starting your project containers"
        },
        {
            "location": "/common-tasks/starting-your-project-containers/#configure-your-environment",
            "text": "Ensure all terminals you intend to use can communicate with the Docker Host.  Generally a simple  docker ps  will either\nlist out running containers or give you an error like  Cannot connect to the Docker daemon. Is the docker daemon running \non this host?  If you get the error make sure you have run  eval \"$(rig config)\" . See  Installation  for the proper way to configure your environments.",
            "title": "Configure your environment"
        },
        {
            "location": "/common-tasks/starting-your-project-containers/#start-your-containers",
            "text": "In the project directory, start the containers with:  docker-compose up   Note  The docker-compose command runs in the foreground as long as the Docker containers are running.  (It is not hung \nif there is no output after \"Attaching [container name]...)   Logs from the running containers will stream to the console and be prefixed by their compose names plus an \ninteger (i.e. web_1, db_1, etc)  You can start another terminal tab if you need to run other commands (such as the build container operations, etc.)",
            "title": "Start your containers"
        },
        {
            "location": "/common-tasks/stopping-containers-and-cleanup/",
            "text": "Stopping containers and cleanup\n\n\nStopping the containers for your project\n\n\nYou may want to recoup the resources used by your projects containers and the docker host for other things if you are \ndone with development for a while.  You can stop the containers for a single project only or all containers and the docker \nhost depending on what you are finished using.\n\n\n\n\nIf you only want to stop the containers for your project, in the project directory, run \ndocker-compose stop\n or \npress Ctrl-C to stop a docker-compose process running in the foreground and then run \ndocker-compose stop\n to ensure \nthe project containers have stopped.\n\n\nIf you want to shut down the docker host as well as any containers, run \nrig stop\n\n\n\n\nCleaning Up\n\n\nFrom time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a \nproject to release all the resources used by it.\n\n\nFor periodic cleanup of all stopped containers, run the following script while your docker host is running: \nrig prune\n\n\nIf you only want to clean up project specific stopped containers, you can run: \ndocker-compose rm\n from your project directory.\n\n\nWhen you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it \nup. The exact directory to request removal from will depend on your project (see suggested directory naming guidelines in \n\nKey Concepts\n): \ndocker ssh dev sudo rm -rf /data/[project]/",
            "title": "Stopping containers and cleanup"
        },
        {
            "location": "/common-tasks/stopping-containers-and-cleanup/#stopping-containers-and-cleanup",
            "text": "",
            "title": "Stopping containers and cleanup"
        },
        {
            "location": "/common-tasks/stopping-containers-and-cleanup/#stopping-the-containers-for-your-project",
            "text": "You may want to recoup the resources used by your projects containers and the docker host for other things if you are \ndone with development for a while.  You can stop the containers for a single project only or all containers and the docker \nhost depending on what you are finished using.   If you only want to stop the containers for your project, in the project directory, run  docker-compose stop  or \npress Ctrl-C to stop a docker-compose process running in the foreground and then run  docker-compose stop  to ensure \nthe project containers have stopped.  If you want to shut down the docker host as well as any containers, run  rig stop",
            "title": "Stopping the containers for your project"
        },
        {
            "location": "/common-tasks/stopping-containers-and-cleanup/#cleaning-up",
            "text": "From time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a \nproject to release all the resources used by it.  For periodic cleanup of all stopped containers, run the following script while your docker host is running:  rig prune  If you only want to clean up project specific stopped containers, you can run:  docker-compose rm  from your project directory.  When you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it \nup. The exact directory to request removal from will depend on your project (see suggested directory naming guidelines in  Key Concepts ):  docker ssh dev sudo rm -rf /data/[project]/",
            "title": "Cleaning Up"
        },
        {
            "location": "/common-tasks/using-the-build-container/",
            "text": "Using the Build Container\n\n\nPart of Outrigger is a \nbuild\n image.  The idea of the build \ncontainer is that it will have installed many of the tools you'd need to work on a project and the proper versions \nso that they work well together.  Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. \nProviding these tools in a container means that you'll never have to worry about having the right tools installed on \nyour laptop or integration environment to get your job done.\n\n\nThe example Drupal project has a \nbuild.yml\n\nfile that shows a variety of ways to use the container.  \n\n\nGetting a build container command line\n\n\nOne of the most common ways to use the build container is to launch a shell into the container and run commands from the \ncommand line interface (cli). The following command get you to the cli\n\n\ndocker-compose -f build.yml run cli\n\n\n\n\nMaking a command alias\n\n\nUsing a shell alias you can make a really quick CLI command that is reusable across projects if you name your build.yml\nfile the same everywhere. \nalias cli='docker-compose -f build.yml run cli\n then you only need to \ncli\n to get into\nyour build containers shell.  This \ntrick\n can be used with all of the command below as well.\n\n\n\n\nRunning a command container\n\n\nA \"command container\" is a term we have coined for a specific approach we use, where containers are spun up to run a single \ncommand in a consistent environment and then shut down.  Here are some example of command containers that we have configured \ninto the \nbuild.yml\n file from our example repository above.\n\n\nDrush Command Container\n\n\nThis is a general purpose Drush command container that allows you to run any Drush command in a consistent environment\n\n\ndocker-compose -f build.yml run drush [command here]\n\n\nGrunt Command Container\n\n\nThis is a general purpose Grunt command container that allows you to run any grunt command in a consistent environment\n\n\ndocker-compose -f build.yml run grunt [command here]\n\n\nComposer Command Container\n\n\nThis is a general purpose Composer command container that allows you to run any composer command in a consistent environment\n\n\ndocker-compose -f build.yml run composer [command here]\n\n\nComposer Install Command Container\n\n\nThis one goes a step further when there is a specific command you want to run and control all the arguments used to run it.\n\n\ndocker-compose -f build.yml run composer-install\n\n\nShare output from the build container\n\n\nUsing the example project from above, we have a volume mount of your project code into the build container. Now, when \nyou perform a build and it writes the output to the local project directory (e.g in ./build/html or something else in \nthe working directory), those changes to the filesystem are effectively outside of the container. The web container should \nalso define a volume mount for the build output into the docroot, or mount your project directory in the container and \nconfigure the docroot to be the absolute path to your build output and you will have seamless integration of build output \nto a web container running your project.",
            "title": "Using the build container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#using-the-build-container",
            "text": "Part of Outrigger is a  build  image.  The idea of the build \ncontainer is that it will have installed many of the tools you'd need to work on a project and the proper versions \nso that they work well together.  Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. \nProviding these tools in a container means that you'll never have to worry about having the right tools installed on \nyour laptop or integration environment to get your job done.  The example Drupal project has a  build.yml \nfile that shows a variety of ways to use the container.",
            "title": "Using the Build Container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#getting-a-build-container-command-line",
            "text": "One of the most common ways to use the build container is to launch a shell into the container and run commands from the \ncommand line interface (cli). The following command get you to the cli  docker-compose -f build.yml run cli   Making a command alias  Using a shell alias you can make a really quick CLI command that is reusable across projects if you name your build.yml\nfile the same everywhere.  alias cli='docker-compose -f build.yml run cli  then you only need to  cli  to get into\nyour build containers shell.  This  trick  can be used with all of the command below as well.",
            "title": "Getting a build container command line"
        },
        {
            "location": "/common-tasks/using-the-build-container/#running-a-command-container",
            "text": "A \"command container\" is a term we have coined for a specific approach we use, where containers are spun up to run a single \ncommand in a consistent environment and then shut down.  Here are some example of command containers that we have configured \ninto the  build.yml  file from our example repository above.",
            "title": "Running a command container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#drush-command-container",
            "text": "This is a general purpose Drush command container that allows you to run any Drush command in a consistent environment  docker-compose -f build.yml run drush [command here]",
            "title": "Drush Command Container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#grunt-command-container",
            "text": "This is a general purpose Grunt command container that allows you to run any grunt command in a consistent environment  docker-compose -f build.yml run grunt [command here]",
            "title": "Grunt Command Container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#composer-command-container",
            "text": "This is a general purpose Composer command container that allows you to run any composer command in a consistent environment  docker-compose -f build.yml run composer [command here]",
            "title": "Composer Command Container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#composer-install-command-container",
            "text": "This one goes a step further when there is a specific command you want to run and control all the arguments used to run it.  docker-compose -f build.yml run composer-install",
            "title": "Composer Install Command Container"
        },
        {
            "location": "/common-tasks/using-the-build-container/#share-output-from-the-build-container",
            "text": "Using the example project from above, we have a volume mount of your project code into the build container. Now, when \nyou perform a build and it writes the output to the local project directory (e.g in ./build/html or something else in \nthe working directory), those changes to the filesystem are effectively outside of the container. The web container should \nalso define a volume mount for the build output into the docroot, or mount your project directory in the container and \nconfigure the docroot to be the absolute path to your build output and you will have seamless integration of build output \nto a web container running your project.",
            "title": "Share output from the build container"
        },
        {
            "location": "/common-tasks/working-with-volumes/",
            "text": "Working with Volumes\n\n\nVolumes are a way that you can map directories or individual files into a running container.  This is useful to provide \ncode to run for a generic container, or directories to store data that persist longer than the life of the container, or \nto even override directories and/or files that exist in an image.  Lets look at a few of those examples.\n\n\nProvide code for a generic container to run*\n\n\nThe default phase2/apache-php:php70 is a Web/PHP container that provides only an index file in \n/var/www/html\n that \nprints out phpinfo().  This is obviously not very useful for an application, so we can provide an entire Drupal site to \nrun and we do that by mapping our site into the default docroot like this:\n\n\n./build/html:/var/www/html\n\n\nThis takes the Drupal site we have in our local project directory of \nbuild/html\n and it overrides the default content \nof the images \n/var/www/html\n directory.\n\n\nPersist data longer than the life of the container\n\n\nWhen using a database like MySQL the container will store the database files in \n/var/lib/mysql\n and by default that \ndirectory will be reset every time the container restarts. This means each time you restart your container you'd need \nto reinstall your application and database, which can make life difficult. So in order to persist MySQL data for longer \nthan the current run of the container we will map a directory from the Docker Host into the container and override the \ndefault \n/var/lib/mysql\n directory. The configuration will look something like this.\n\n\n/data/drupal/mysql:/var/lib/mysql\n\n\nNow when your database container creates files in \n/var/lib/mysql\n they are actually saved in the \n/data/drupal/mysql\n \ndirectory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict \nwith each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all \nof your disk space. See the cleaning up section for more info.\n\n\nOverride directories and files that exist in an image\n\n\nGenerally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that \nconfiguration is not suitable for development and we need to override configurations.  With volumes we have shown \nearlier how you can override directories, but you can also override individual files too.\n\n\n./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf\n\n\nThis takes a local \nhttpd.conf\n file from our project and overrides the \n/etc/httpd/httpd.conf\n files that ships with \nthe container. \n\n\nChanging Volume Definitions in Compose File\n\n\nIf you wind up changing volume definitions in your docker-compose file you will need to remove your container before it \nwill recognize those changes on a restart.\n\n\nIn the project directory, run: \ndocker-compose rm\n\n\nThis command will remove all stopped containers defined in the docker-compose.yml.  You could also remove an individual \ncontainer by running: \ndocker-compose rm \nname in yml\n\n\nAfter removing the container, it will revert to the original state from the Docker image, removing any packages \ninstalled or files modified that are not included in a volume mount. Start your containers again with \ndocker-compose up\n \nand you should have your new volume mounts.",
            "title": "Working with Volumes"
        },
        {
            "location": "/common-tasks/working-with-volumes/#working-with-volumes",
            "text": "Volumes are a way that you can map directories or individual files into a running container.  This is useful to provide \ncode to run for a generic container, or directories to store data that persist longer than the life of the container, or \nto even override directories and/or files that exist in an image.  Lets look at a few of those examples.",
            "title": "Working with Volumes"
        },
        {
            "location": "/common-tasks/working-with-volumes/#provide-code-for-a-generic-container-to-run",
            "text": "The default phase2/apache-php:php70 is a Web/PHP container that provides only an index file in  /var/www/html  that \nprints out phpinfo().  This is obviously not very useful for an application, so we can provide an entire Drupal site to \nrun and we do that by mapping our site into the default docroot like this:  ./build/html:/var/www/html  This takes the Drupal site we have in our local project directory of  build/html  and it overrides the default content \nof the images  /var/www/html  directory.",
            "title": "Provide code for a generic container to run*"
        },
        {
            "location": "/common-tasks/working-with-volumes/#persist-data-longer-than-the-life-of-the-container",
            "text": "When using a database like MySQL the container will store the database files in  /var/lib/mysql  and by default that \ndirectory will be reset every time the container restarts. This means each time you restart your container you'd need \nto reinstall your application and database, which can make life difficult. So in order to persist MySQL data for longer \nthan the current run of the container we will map a directory from the Docker Host into the container and override the \ndefault  /var/lib/mysql  directory. The configuration will look something like this.  /data/drupal/mysql:/var/lib/mysql  Now when your database container creates files in  /var/lib/mysql  they are actually saved in the  /data/drupal/mysql  \ndirectory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict \nwith each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all \nof your disk space. See the cleaning up section for more info.",
            "title": "Persist data longer than the life of the container"
        },
        {
            "location": "/common-tasks/working-with-volumes/#override-directories-and-files-that-exist-in-an-image",
            "text": "Generally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that \nconfiguration is not suitable for development and we need to override configurations.  With volumes we have shown \nearlier how you can override directories, but you can also override individual files too.  ./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf  This takes a local  httpd.conf  file from our project and overrides the  /etc/httpd/httpd.conf  files that ships with \nthe container.",
            "title": "Override directories and files that exist in an image"
        },
        {
            "location": "/common-tasks/working-with-volumes/#changing-volume-definitions-in-compose-file",
            "text": "If you wind up changing volume definitions in your docker-compose file you will need to remove your container before it \nwill recognize those changes on a restart.  In the project directory, run:  docker-compose rm  This command will remove all stopped containers defined in the docker-compose.yml.  You could also remove an individual \ncontainer by running:  docker-compose rm  name in yml  After removing the container, it will revert to the original state from the Docker image, removing any packages \ninstalled or files modified that are not included in a volume mount. Start your containers again with  docker-compose up  \nand you should have your new volume mounts.",
            "title": "Changing Volume Definitions in Compose File"
        },
        {
            "location": "/common-tasks/working-offline/",
            "text": "Working Offline\n\n\nA common desire is the ability to leverage the full development environment provided to work\noffline. When attempting this users are often thwarted trying to access containers which\nappear to be successfully running.\n\n\nThis issue is due to a failure to resolve domain names to the appropriate IP address for the\ncontainer. This affects those using OS X and appears to be the result of a failure of the name\nresolution system to operate when no network interface appears connected. Typical error messages\nin browsers warn of being disconnected from the internet. Specifically, Chrome will include the \nerror code ERR_INTERNET_DISCONNECTED. In these instances, a test command like \n\ncurl -v http://www.project.vm/\n succeeds as does attempting to use the IP address of a container\nwithin a browser. The command \nrig dns-records\n will display a mapping between all known\ncontainers and IP addresses. The \ndocker inspect CONTAINER_NAME\n command can be used to view more\ndetailed information about a specific container including its IP address and should be used on\nsystems not supporting the \nrig\n application.\n\n\nOffline DNS Workaround\n\n\nThe work around for this issue is to use the \nrig dns-records\n command and copy the output\nto your \n/etc/hosts\n file. You'll need to update this file any time you start or stop project\ncontainers and you should clean entries from it when you reconnect to a network. On systems\nwhich do not support the \nrig\n application the \ndocker inspect\n command can be used to\nbuild a mapping of IP addresses to domain names.\n\n\n\n\nNetwork Changes Can Require Restart\n\n\nNetwork changes such as connecting or disconnecting an interface or VPN can require a restart\nof your Outrigger environment via \nrig restart\n or a re-execution of the \nrig dns\n\ncommand to ensure routing of traffic to containers and DNS resolution is properly configured.\n\n\n\n\nAdditional Information\n\n\nThose seeking additional information about the root cause of this issue and wishing to explore\npotential solutions can read further information at the following URLs. Note that none of the\npurported solutions other than that described above has proven successful in testing. Note that\nsome of these links refer to DNS utilities used in older OS X versions.\n\n\n\n\nhttp://serverfault.com/questions/22419/set-dns-server-on-os-x-even-when-without-internet-connection\n\n\nhttp://apple.stackexchange.com/questions/202887/mac-doesnt-use-local-dns-for-local\n\n\nhttp://superuser.com/questions/418833/using-dnsmasq-on-os-x-when-not-connected-to-the-internet\n\n\nhttp://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x\n\n\n\n\nThis same issue affects other projects as well.\n\n\n\n\nhttps://github.com/basecamp/pow/issues/471\n\n\nhttps://github.com/basecamp/pow/issues/104\n\n\n\n\nA theoretical solution to this issue would be to have a network interface that always appeared\nactive triggering the attempt to resolve the domain name. The following post discusses creation\nof a virtual interface to satisfy this requirement. The mechanism mentioned by bmasterswizzle and\nAlex Gray did not prove effective in testing. The interface could be created successfully but\nnever brought up to a state where OS X considered it active.\n\n\n\n\nhttp://stackoverflow.com/questions/87442/virtual-network-interface-in-mac-os-x\n\n\n\n\nSome additional information and demonstrations of utilities for diagnostics can be found at\n\n\n\n\nhttp://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x\n\n\nhttp://serverfault.com/questions/478534/how-is-dns-lookup-configured-for-osx-mountain-lion",
            "title": "Working Offline"
        },
        {
            "location": "/common-tasks/working-offline/#working-offline",
            "text": "A common desire is the ability to leverage the full development environment provided to work\noffline. When attempting this users are often thwarted trying to access containers which\nappear to be successfully running.  This issue is due to a failure to resolve domain names to the appropriate IP address for the\ncontainer. This affects those using OS X and appears to be the result of a failure of the name\nresolution system to operate when no network interface appears connected. Typical error messages\nin browsers warn of being disconnected from the internet. Specifically, Chrome will include the \nerror code ERR_INTERNET_DISCONNECTED. In these instances, a test command like  curl -v http://www.project.vm/  succeeds as does attempting to use the IP address of a container\nwithin a browser. The command  rig dns-records  will display a mapping between all known\ncontainers and IP addresses. The  docker inspect CONTAINER_NAME  command can be used to view more\ndetailed information about a specific container including its IP address and should be used on\nsystems not supporting the  rig  application.",
            "title": "Working Offline"
        },
        {
            "location": "/common-tasks/working-offline/#offline-dns-workaround",
            "text": "The work around for this issue is to use the  rig dns-records  command and copy the output\nto your  /etc/hosts  file. You'll need to update this file any time you start or stop project\ncontainers and you should clean entries from it when you reconnect to a network. On systems\nwhich do not support the  rig  application the  docker inspect  command can be used to\nbuild a mapping of IP addresses to domain names.   Network Changes Can Require Restart  Network changes such as connecting or disconnecting an interface or VPN can require a restart\nof your Outrigger environment via  rig restart  or a re-execution of the  rig dns \ncommand to ensure routing of traffic to containers and DNS resolution is properly configured.",
            "title": "Offline DNS Workaround"
        },
        {
            "location": "/common-tasks/working-offline/#additional-information",
            "text": "Those seeking additional information about the root cause of this issue and wishing to explore\npotential solutions can read further information at the following URLs. Note that none of the\npurported solutions other than that described above has proven successful in testing. Note that\nsome of these links refer to DNS utilities used in older OS X versions.   http://serverfault.com/questions/22419/set-dns-server-on-os-x-even-when-without-internet-connection  http://apple.stackexchange.com/questions/202887/mac-doesnt-use-local-dns-for-local  http://superuser.com/questions/418833/using-dnsmasq-on-os-x-when-not-connected-to-the-internet  http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x   This same issue affects other projects as well.   https://github.com/basecamp/pow/issues/471  https://github.com/basecamp/pow/issues/104   A theoretical solution to this issue would be to have a network interface that always appeared\nactive triggering the attempt to resolve the domain name. The following post discusses creation\nof a virtual interface to satisfy this requirement. The mechanism mentioned by bmasterswizzle and\nAlex Gray did not prove effective in testing. The interface could be created successfully but\nnever brought up to a state where OS X considered it active.   http://stackoverflow.com/questions/87442/virtual-network-interface-in-mac-os-x   Some additional information and demonstrations of utilities for diagnostics can be found at   http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x  http://serverfault.com/questions/478534/how-is-dns-lookup-configured-for-osx-mountain-lion",
            "title": "Additional Information"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/",
            "text": "SSH into a Container\n\n\nHow do I SSH into a running container\n\n\nThere is a docker exec command that can be used to connect to a container that is already running.  \n\n\n\n\nUse \ndocker ps\n to get the name of the existing container\n\n\nUse the command \ndocker exec -it \ncontainer name\n /bin/bash\n to get a bash shell in the container\n\n\nGenerically, use \ndocker exec -it \ncontainer name\n \ncommand\n to execute whatever command you specify in the container.\n\n\n\n\nHow do I run a command in my container\n\n\nThe proper way to run a command in a container is: \ndocker-compose run \ncontainer name\n \ncommand\n. For example, to get \na shell into your web container you might run \ndocker-compose run web /bin/bash\n\n\nTo run a series of commands, you must wrap them in a single command using a shell. For example: \n\n\ndocker-compose run [name in yml] sh -c '[command 1] \n [command 2] \n [command 3]'\n\n\n\n\nIn some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new \ncontainer configuration. Use docker run to start a new container with a given image: \ndocker run -it \nimage name\n \ncommand\n\n\nThe docker run command accepts command line options to specify volume mounts, environment variables, the working \ndirectory, and more.\n\n\nGetting a shell for build/tooling operations\n\n\nGetting a shell into a build container to execute any operations is the simplest approach. You simply want to get access \nto the \ncli\n container we defined in the compose file.  The command \ndocker-compose -f build.yml run cli\n will start an \ninstance of the \noutrigger/build\n image and run a bash shell for you.  From there you are free to use \ndrush\n, \n\ngrunt\n or whatever your little heart desires.\n\n\nRunning commands, but not from a dedicated shell\n\n\nAnother concept in the Docker world is starting a container to run a single command and allowing the container stop when \nthe command is completed.  This is great if you run commands infrequently, or don't want to have another container \nconstantly running.  Running your commands on containers in this fashion is also well suited for commands that don't \ngenerate any files on the filesystem or if they do, they write those files on to volumes mounted into the container.\n\n\nThe \ndrush\n container defined in the example \nbuild.yml\n file is a container designed specifically to run Drush in a \nsingle working directory taking only the commands as arguments.  This approach allows us to provide a quick and easy \nmechanism for running any Drush command, such as \nsqlc\n, \ncache-rebuild\n, and others, in your Drupal site quick and easily.\n\n\nThere are also other examples of a \ngrunt\n command container similar to \ndrush\n and an even more specific command \ncontainer around running a single command, \ndrush make\n to build the site from a make/dependency file.",
            "title": "SSH into a container"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/#ssh-into-a-container",
            "text": "",
            "title": "SSH into a Container"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/#how-do-i-ssh-into-a-running-container",
            "text": "There is a docker exec command that can be used to connect to a container that is already running.     Use  docker ps  to get the name of the existing container  Use the command  docker exec -it  container name  /bin/bash  to get a bash shell in the container  Generically, use  docker exec -it  container name   command  to execute whatever command you specify in the container.",
            "title": "How do I SSH into a running container"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/#how-do-i-run-a-command-in-my-container",
            "text": "The proper way to run a command in a container is:  docker-compose run  container name   command . For example, to get \na shell into your web container you might run  docker-compose run web /bin/bash  To run a series of commands, you must wrap them in a single command using a shell. For example:   docker-compose run [name in yml] sh -c '[command 1]   [command 2]   [command 3]'  In some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new \ncontainer configuration. Use docker run to start a new container with a given image:  docker run -it  image name   command  The docker run command accepts command line options to specify volume mounts, environment variables, the working \ndirectory, and more.",
            "title": "How do I run a command in my container"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/#getting-a-shell-for-buildtooling-operations",
            "text": "Getting a shell into a build container to execute any operations is the simplest approach. You simply want to get access \nto the  cli  container we defined in the compose file.  The command  docker-compose -f build.yml run cli  will start an \ninstance of the  outrigger/build  image and run a bash shell for you.  From there you are free to use  drush ,  grunt  or whatever your little heart desires.",
            "title": "Getting a shell for build/tooling operations"
        },
        {
            "location": "/common-tasks/ssh-into-a-container/#running-commands-but-not-from-a-dedicated-shell",
            "text": "Another concept in the Docker world is starting a container to run a single command and allowing the container stop when \nthe command is completed.  This is great if you run commands infrequently, or don't want to have another container \nconstantly running.  Running your commands on containers in this fashion is also well suited for commands that don't \ngenerate any files on the filesystem or if they do, they write those files on to volumes mounted into the container.  The  drush  container defined in the example  build.yml  file is a container designed specifically to run Drush in a \nsingle working directory taking only the commands as arguments.  This approach allows us to provide a quick and easy \nmechanism for running any Drush command, such as  sqlc ,  cache-rebuild , and others, in your Drupal site quick and easily.  There are also other examples of a  grunt  command container similar to  drush  and an even more specific command \ncontainer around running a single command,  drush make  to build the site from a make/dependency file.",
            "title": "Running commands, but not from a dedicated shell"
        },
        {
            "location": "/common-tasks/customizing-container-configuration/",
            "text": "Customizing Configuration\n\n\nThere are a few primary mechanisms for customizing the configuration of a container.\n\n\nEnvironment Variables\n\n\nMany images will react to environmental variables to alter their behavior. Images which offer\nthis functionality should document it in the README for the image. For an example see the\n\nOutrigger Build\n image. When environmental\nconfiguration is available it can be triggered by specifying the appropriate value in the\nenvironments section of your docker compose file.\n\n\nwww:\n  image: phase2/apache-php:php56\n  environment:\n    # Change some core container settings\n    PHP_MAX_EXECUTION_TIME: 45\n    # These enable debug/profiling support\n    PHP_XDEBUG: \ntrue\n\n    PHP_XHPROF: \nfalse\n\n\n\n\n\nVolume Mounts\n\n\nIf the container image you are using doesn't allow for change via environmental variables, your \nnext option is to override the configuration file using volume mounting. In this setup, you\nreplace the configuration file inside a container with a file from your host machine.\n\n\nwww:\n  image: phase2/apache-php:php56\n  volumes:\n    # substitute in a special mime magic file because our project handles files\n    # of special types\n    - ./env/www/etc/httpd/conf/magic:/etc/httpd/conf/magic\n\n\n\n\nVolume Mounts for confd\n\n\nPhase2 images often use \nconfd\n to template configuration from environment variable and other sources.\nThe configuration file you may be trying to override might be generated on container initiation via confd. If this is the \ncase, you'll need to override the template file from which the configuration file is created. If you find that your copy \nof the configuration file on your host machine is updated when you start a container this is likely the cause. \n\n\nFor example, the Xdebug configuration provided in our \nphase2/apache-php:php70\n container is provided by confd, so this\nis how you would override that configuration\n\n\nwww:\n  image: phase2/apache-php:php70\n  volumes:\n    # substitute a different confd template file into the image so confd will use the override on container boot\n    - ./env/www/etc/confd/templates/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl",
            "title": "Changing container configuration"
        },
        {
            "location": "/common-tasks/customizing-container-configuration/#customizing-configuration",
            "text": "There are a few primary mechanisms for customizing the configuration of a container.",
            "title": "Customizing Configuration"
        },
        {
            "location": "/common-tasks/customizing-container-configuration/#environment-variables",
            "text": "Many images will react to environmental variables to alter their behavior. Images which offer\nthis functionality should document it in the README for the image. For an example see the Outrigger Build  image. When environmental\nconfiguration is available it can be triggered by specifying the appropriate value in the\nenvironments section of your docker compose file.  www:\n  image: phase2/apache-php:php56\n  environment:\n    # Change some core container settings\n    PHP_MAX_EXECUTION_TIME: 45\n    # These enable debug/profiling support\n    PHP_XDEBUG:  true \n    PHP_XHPROF:  false",
            "title": "Environment Variables"
        },
        {
            "location": "/common-tasks/customizing-container-configuration/#volume-mounts",
            "text": "If the container image you are using doesn't allow for change via environmental variables, your \nnext option is to override the configuration file using volume mounting. In this setup, you\nreplace the configuration file inside a container with a file from your host machine.  www:\n  image: phase2/apache-php:php56\n  volumes:\n    # substitute in a special mime magic file because our project handles files\n    # of special types\n    - ./env/www/etc/httpd/conf/magic:/etc/httpd/conf/magic",
            "title": "Volume Mounts"
        },
        {
            "location": "/common-tasks/customizing-container-configuration/#volume-mounts-for-confd",
            "text": "Phase2 images often use  confd  to template configuration from environment variable and other sources.\nThe configuration file you may be trying to override might be generated on container initiation via confd. If this is the \ncase, you'll need to override the template file from which the configuration file is created. If you find that your copy \nof the configuration file on your host machine is updated when you start a container this is likely the cause.   For example, the Xdebug configuration provided in our  phase2/apache-php:php70  container is provided by confd, so this\nis how you would override that configuration  www:\n  image: phase2/apache-php:php70\n  volumes:\n    # substitute a different confd template file into the image so confd will use the override on container boot\n    - ./env/www/etc/confd/templates/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl",
            "title": "Volume Mounts for confd"
        },
        {
            "location": "/common-tasks/using-watches/",
            "text": "Using Watches\n\n\nTODO: Rework to better explain that NFS is there and easy, but not notifications. Unison requires some setup and moving \npieces but supports full filesystem notifications and two way syncing. If you need file system notifications, you much \nuse unison.\n\n\nOften we need \nwatches\n running inside of our containers. This could be for webpack, grunt, nodemon, etc.  One of the \nmain challenges with the NFS mounts used in Outrigger is that they do not forward filesystem notifications across the \nNFS mount and into containers, so we need to facilitate that.\n\n\nrig\n now supports unison based file syncing between the local filesystem and a volume shared with a container. This\nsetup allows multi-directional file syncing as well as full support for all filesystem notifications. \n\n\nSee \nFilesystem Sync\n for details on how to setup Unison volumes.",
            "title": "Using watches"
        },
        {
            "location": "/common-tasks/using-watches/#using-watches",
            "text": "TODO: Rework to better explain that NFS is there and easy, but not notifications. Unison requires some setup and moving \npieces but supports full filesystem notifications and two way syncing. If you need file system notifications, you much \nuse unison.  Often we need  watches  running inside of our containers. This could be for webpack, grunt, nodemon, etc.  One of the \nmain challenges with the NFS mounts used in Outrigger is that they do not forward filesystem notifications across the \nNFS mount and into containers, so we need to facilitate that.  rig  now supports unison based file syncing between the local filesystem and a volume shared with a container. This\nsetup allows multi-directional file syncing as well as full support for all filesystem notifications.   See  Filesystem Sync  for details on how to setup Unison volumes.",
            "title": "Using Watches"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/",
            "text": "Local Xdebug with Outrigger \n PhpStorm\n\n\nGetting \nXdebug\n set up can be a bit challenging but while there are many discrete steps, they are \nindividually straightforward. This guide will walk you through getting setup quickly with PhpStorm.\n\n\n\n\nApplies to use of the Phase2 Docker Images\n\n\nThis documentation specifically pertains to using Phase2's \n\nApache-PHP Docker Images\n \nor the Phase2 \nBuild Image\n.\n\n\nrig\n itself is only relevant in that it brokers standardized DNS practices.\n\n\n\n\n\n\nMake sure your environment is up-to-date\n\n\nIn case there might be fixes for any problems you might encounter, consider \nupdating rig\n before \nproceeding.\n\n\nOnce done, run \nrig doctor\n to confirm Outrigger is in a healthy. Check out \nTroubleshooting\n \nor the \nF.A.Q.\n if anything comes up.\n\n\nIf you haven't updated your Docker Images in awhile, doing so now is a good precautionary step that you have everything \nyou need. Check out the \nRoutine Image Maintenance\n\n\n\n\nSetup Steps\n\n\n1. Activate Xdebug for your running Drupal site\n\n\nIn your docker command or your docker-compose.yml manifest, ensure the environment variable \nPHP_XDEBUG=\"true\"\n. This \nwill load the PHP Xdebug extension with the default configuration.\n\n\nFor details of the Xdebug configuration of Phase2's Apache PHP containers, check out the \n\napache-php-base Docker Hub page\n.\n\n\n2. Configure PhpStorm for Xdebug\n\n\nTo get started configuring your PhpStorm IDE open the application settings.\n\n\nClick on the wrench icon in the toolbar:\n\n\n\n\nYou can also get to the project settings by going to: PhpStorm \n Preferences (OS X) or File \n Settings (Windows, Linux).\n\n\n3. Adjust the PHP Project settings\n\n\nMake sure you have the correct version of PHP selected:\n\n\n\n\n4. Adjust the Debug Project settings\n\n\n\n\nXdebug is using Port 9000.\n\n\nAccept external connections.\n\n\n\n\n\n\n\n\nEyes on Your Xdebug Configuration\n\n\nYou can view your Xdebug configuration by looking inside the Apache container.\nWith the container name (found via \ndocker ps\n), try running:\n\n\ndocker exec [container_name] /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini\n\n\nif using docker-compose with your Apache container named *\nwww\n, you can more simply run:\n\n\ndocker-compose exec www /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini\n\n\nThis path varies by PHP version. For PHP 5.6 check \n/etc/opt/rh/rh-php56/php.d/15-xdebug.ini\n.\n\n\n\n\n5. For the DBGp Proxy, just ensure that the port is the same\n\n\nYou can leave the other settings blank.\n\n\n\n\n6. Adjust the Server Project settings\n\n\nCreate a new Server by clicking on the \"+\" button. Give your server a name and input the host.\n\n\nBe sure to add the docroot mappings. The example shown here is using the \n\nGrunt Drupal Tasks\n project structure. There are two mappings in this \ncase. One for the docroot (\nbuild/html\n) and the other for the \nsrc\n directory so that breakpoints can be set in the \ncustom modules in the \nsrc\n directory as well.\n\n\nThese mappings are used to match paths from inside the Docker container to the paths\nused in the local filesystem where PhpStorm is run.\n\n\n\n\n7. Validate your debug settings\n\n\n\n\nProxies Interfere with Xdebug\n\n\nWhen setting your server URL, be sure to use the URL associated with your web container. If you are using a proxy \n(such as Varnish) that URL may validate but will not work in practice.\n\n\n\n\nSelect the \"Web Server Debug Validation\" option from the \"Run\" menu option. (Confirm your Apache container is running \nor this validation will fail.)\n\n\n\n\nThis will display a dialog window that allows you to validate your settings. Make sure that your \"Path to create \nvalidation script\" points to your project docroot and the URL is your project URL.\n\n\nIf all goes well, clicking the \"Validate\" button should give you something like this:\n\n\n\n\nClick the dialog 'x' (close) button to close this dialog window.\n\n\n8. Restart PhpStorm\n\n\nIn order to ensure that all your settings are applying, you will need to restart PhpStorm.\n\n\n9. Make sure that you listen for connections\n\n\n\n\n10. Configure a Debugger\n\n\n\n\nSelect Run -\n Edit Configurations from the main PhpStorm menu\n\n\nClick + and add a \"PHP Web Application\"\n\n\nGive it a name, then select the Server defined in the previous section from the drop-down menu.\n\n\nEnter a Start URL of \n/\n.\n\n\nSelect your Browser (e.g. Chrome)\n\n\nClick \nOK\n.\n\n\n\n\n\n\nOverriding the Default Xdebug Configuration\n\n\nIf your project or workflow has special needs, you can override the Xdebug configuration\nusing Volume Mounts to substitute your own template file. Copy \n\nthe original template\n \ninto your project and make the necessary changes. (You can also pull your current version of this file from the locally \nrunning docker image.)\n\n\nCommit your version of the file and add a volume mount to your docker-compose manifest with an entry such as:\n\n\n./env/local/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl\n\n\nOnce that's in place, you will have to restart the container to pick up the new volume mount:\n\n\ndocker-compose restart www",
            "title": "Using Xdebug with PHPStorm"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#local-xdebug-with-outrigger-phpstorm",
            "text": "Getting  Xdebug  set up can be a bit challenging but while there are many discrete steps, they are \nindividually straightforward. This guide will walk you through getting setup quickly with PhpStorm.   Applies to use of the Phase2 Docker Images  This documentation specifically pertains to using Phase2's  Apache-PHP Docker Images  \nor the Phase2  Build Image .  rig  itself is only relevant in that it brokers standardized DNS practices.    Make sure your environment is up-to-date  In case there might be fixes for any problems you might encounter, consider  updating rig  before \nproceeding.  Once done, run  rig doctor  to confirm Outrigger is in a healthy. Check out  Troubleshooting  \nor the  F.A.Q.  if anything comes up.  If you haven't updated your Docker Images in awhile, doing so now is a good precautionary step that you have everything \nyou need. Check out the  Routine Image Maintenance",
            "title": "Local Xdebug with Outrigger &amp; PhpStorm"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#setup-steps",
            "text": "",
            "title": "Setup Steps"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#1-activate-xdebug-for-your-running-drupal-site",
            "text": "In your docker command or your docker-compose.yml manifest, ensure the environment variable  PHP_XDEBUG=\"true\" . This \nwill load the PHP Xdebug extension with the default configuration.  For details of the Xdebug configuration of Phase2's Apache PHP containers, check out the  apache-php-base Docker Hub page .",
            "title": "1. Activate Xdebug for your running Drupal site"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#2-configure-phpstorm-for-xdebug",
            "text": "To get started configuring your PhpStorm IDE open the application settings.  Click on the wrench icon in the toolbar:   You can also get to the project settings by going to: PhpStorm   Preferences (OS X) or File   Settings (Windows, Linux).",
            "title": "2. Configure PhpStorm for Xdebug"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#3-adjust-the-php-project-settings",
            "text": "Make sure you have the correct version of PHP selected:",
            "title": "3. Adjust the PHP Project settings"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#4-adjust-the-debug-project-settings",
            "text": "Xdebug is using Port 9000.  Accept external connections.     Eyes on Your Xdebug Configuration  You can view your Xdebug configuration by looking inside the Apache container.\nWith the container name (found via  docker ps ), try running:  docker exec [container_name] /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini  if using docker-compose with your Apache container named * www , you can more simply run:  docker-compose exec www /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini  This path varies by PHP version. For PHP 5.6 check  /etc/opt/rh/rh-php56/php.d/15-xdebug.ini .",
            "title": "4. Adjust the Debug Project settings"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#5-for-the-dbgp-proxy-just-ensure-that-the-port-is-the-same",
            "text": "You can leave the other settings blank.",
            "title": "5. For the DBGp Proxy, just ensure that the port is the same"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#6-adjust-the-server-project-settings",
            "text": "Create a new Server by clicking on the \"+\" button. Give your server a name and input the host.  Be sure to add the docroot mappings. The example shown here is using the  Grunt Drupal Tasks  project structure. There are two mappings in this \ncase. One for the docroot ( build/html ) and the other for the  src  directory so that breakpoints can be set in the \ncustom modules in the  src  directory as well.  These mappings are used to match paths from inside the Docker container to the paths\nused in the local filesystem where PhpStorm is run.",
            "title": "6. Adjust the Server Project settings"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#7-validate-your-debug-settings",
            "text": "Proxies Interfere with Xdebug  When setting your server URL, be sure to use the URL associated with your web container. If you are using a proxy \n(such as Varnish) that URL may validate but will not work in practice.   Select the \"Web Server Debug Validation\" option from the \"Run\" menu option. (Confirm your Apache container is running \nor this validation will fail.)   This will display a dialog window that allows you to validate your settings. Make sure that your \"Path to create \nvalidation script\" points to your project docroot and the URL is your project URL.  If all goes well, clicking the \"Validate\" button should give you something like this:   Click the dialog 'x' (close) button to close this dialog window.",
            "title": "7. Validate your debug settings"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#8-restart-phpstorm",
            "text": "In order to ensure that all your settings are applying, you will need to restart PhpStorm.",
            "title": "8. Restart PhpStorm"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#9-make-sure-that-you-listen-for-connections",
            "text": "",
            "title": "9. Make sure that you listen for connections"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#10-configure-a-debugger",
            "text": "Select Run -  Edit Configurations from the main PhpStorm menu  Click + and add a \"PHP Web Application\"  Give it a name, then select the Server defined in the previous section from the drop-down menu.  Enter a Start URL of  / .  Select your Browser (e.g. Chrome)  Click  OK .",
            "title": "10. Configure a Debugger"
        },
        {
            "location": "/common-tasks/using-xdebug-with-phpstorm/#overriding-the-default-xdebug-configuration",
            "text": "If your project or workflow has special needs, you can override the Xdebug configuration\nusing Volume Mounts to substitute your own template file. Copy  the original template  \ninto your project and make the necessary changes. (You can also pull your current version of this file from the locally \nrunning docker image.)  Commit your version of the file and add a volume mount to your docker-compose manifest with an entry such as:  ./env/local/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl  Once that's in place, you will have to restart the container to pick up the new volume mount:  docker-compose restart www",
            "title": "Overriding the Default Xdebug Configuration"
        },
        {
            "location": "/common-tasks/accessing-logs/",
            "text": "Accessing Logs\n\n\nViewing logs from my container services\n\n\nWhen you start a your containers via \ndocker-compose up\n all of the defined services will start in the foreground. All \nlog output to stdout/stderr within each container will be output the console.  Each entry will be prefixed with the name \nof the running container to identify the source of the log message.\n\n\nIf log output is not coming directly to the console you can \n\nSSH into the container\n and browse the file system for logs. Most common \nservices will provide some log output in \n/var/log\n.",
            "title": "Accessing logs"
        },
        {
            "location": "/common-tasks/accessing-logs/#accessing-logs",
            "text": "",
            "title": "Accessing Logs"
        },
        {
            "location": "/common-tasks/accessing-logs/#viewing-logs-from-my-container-services",
            "text": "When you start a your containers via  docker-compose up  all of the defined services will start in the foreground. All \nlog output to stdout/stderr within each container will be output the console.  Each entry will be prefixed with the name \nof the running container to identify the source of the log message.  If log output is not coming directly to the console you can  SSH into the container  and browse the file system for logs. Most common \nservices will provide some log output in  /var/log .",
            "title": "Viewing logs from my container services"
        },
        {
            "location": "/common-tasks/inter-vm-communication/",
            "text": "Accessing containers from other Virtual Machines\n\n\nIf you use virtual machines to perform cross browser testing you'll need to know\nhow to configure VirtualBox networking to allow for communication with your\ncontainers.\n\n\nNetworking Mode\n\n\nIn your virtual machine settings, under \nNetwork\n choose an enabled adapter\nand then choose NAT for the \nAttached to\n option.\n\n\n\n\nThis will ensure that processes from within virtual machine can route requests\nthrough the network to your containers.\n\n\nDNS\n\n\nIn addition to the network setup, you'll need to choose one of the following\noptions so that the DNS entries for your containers can resolve to the proper IP\naddress.\n\n\nOutrigger DNS\n\n\nThe preferred option is to configure your virtual machine so that it uses the\nDNS services set up by Outrigger. To do so, set 172.17.0.1 as a DNS server in\nthe virtual machine.\n\n\nFor Windows, you can use \nthese instructions for using Google's Public DNS\n\nto navigate to the correct area for setting your DNS server. Use 172.17.0.1\ninstead of the Google IP addresses.\n\n\nFor Linux systems, refer to the \nLinux DNS configuration options\n section of\nthe \nLinux Installation instructions\n\nand choose the option that best works for your system.\n\n\nHosts File\n\n\nIf configuring DNS isn't a viable option for your virtual machine, a backup\napproach is to manually manage the mapping of container domain names to\nthe appropriate IP address. To do this, any time you start or stop containers\nyou will need to take the output of \nrig dns-records\n and update the\n\nhosts file\n in the virtual machine.",
            "title": "Accessing containers from other VMs"
        },
        {
            "location": "/common-tasks/inter-vm-communication/#accessing-containers-from-other-virtual-machines",
            "text": "If you use virtual machines to perform cross browser testing you'll need to know\nhow to configure VirtualBox networking to allow for communication with your\ncontainers.",
            "title": "Accessing containers from other Virtual Machines"
        },
        {
            "location": "/common-tasks/inter-vm-communication/#networking-mode",
            "text": "In your virtual machine settings, under  Network  choose an enabled adapter\nand then choose NAT for the  Attached to  option.   This will ensure that processes from within virtual machine can route requests\nthrough the network to your containers.",
            "title": "Networking Mode"
        },
        {
            "location": "/common-tasks/inter-vm-communication/#dns",
            "text": "In addition to the network setup, you'll need to choose one of the following\noptions so that the DNS entries for your containers can resolve to the proper IP\naddress.",
            "title": "DNS"
        },
        {
            "location": "/common-tasks/inter-vm-communication/#outrigger-dns",
            "text": "The preferred option is to configure your virtual machine so that it uses the\nDNS services set up by Outrigger. To do so, set 172.17.0.1 as a DNS server in\nthe virtual machine.  For Windows, you can use  these instructions for using Google's Public DNS \nto navigate to the correct area for setting your DNS server. Use 172.17.0.1\ninstead of the Google IP addresses.  For Linux systems, refer to the  Linux DNS configuration options  section of\nthe  Linux Installation instructions \nand choose the option that best works for your system.",
            "title": "Outrigger DNS"
        },
        {
            "location": "/common-tasks/inter-vm-communication/#hosts-file",
            "text": "If configuring DNS isn't a viable option for your virtual machine, a backup\napproach is to manually manage the mapping of container domain names to\nthe appropriate IP address. To do this, any time you start or stop containers\nyou will need to take the output of  rig dns-records  and update the hosts file  in the virtual machine.",
            "title": "Hosts File"
        },
        {
            "location": "/common-tasks/ssh-keys-for-private-repos/",
            "text": "SSH Keys for Private Repositories\n\n\nForward/Import your SSH Key into the build container to clone private repositories\n\n\nCertain containers like Jenkins and build may need your private key. This is supported by importing your \nprivate key into the container via a volume mount.  \n\n\nTo get your private key into the build container, volume mount your key into the container at \n/root/.ssh/outrigger.key\n \nand it will be processed accordingly.\n\n\n~/.ssh/id_rsa:/root/.ssh/outrigger.key",
            "title": "SSH Keys for private repos"
        },
        {
            "location": "/common-tasks/ssh-keys-for-private-repos/#ssh-keys-for-private-repositories",
            "text": "",
            "title": "SSH Keys for Private Repositories"
        },
        {
            "location": "/common-tasks/ssh-keys-for-private-repos/#forwardimport-your-ssh-key-into-the-build-container-to-clone-private-repositories",
            "text": "Certain containers like Jenkins and build may need your private key. This is supported by importing your \nprivate key into the container via a volume mount.    To get your private key into the build container, volume mount your key into the container at  /root/.ssh/outrigger.key  \nand it will be processed accordingly.  ~/.ssh/id_rsa:/root/.ssh/outrigger.key",
            "title": "Forward/Import your SSH Key into the build container to clone private repositories"
        },
        {
            "location": "/common-tasks/setting-up-mail/",
            "text": "Setting Up Mail\n\n\nEmail is something that many projects need, but during development you likely do \nnot want to actually send email, but you'd rather have sent mail captured for examination\nand released to a real mail server only in certain situations.\n\n\nTo handle these and other situations we recommend \nMailhog\n\n\nUsing MailHog\n\n\nMailHog can be added as another service with your projects Docker Compose file.\n\n\nSee the mail service defined in the \nOutrigger Example Mail Project\n\nthat service can be copied into your projects \ndocker-compose.yml\n file, or kept \nseparate and started as needed.\n\n\nUsing the configuration from the example you would configure your application \nto use \nmail.outrigger.vm:1025\n as your SMTP server and you could view captured \nmail (and choose to release it) via the web interface accessible at \nhttp://mail.outrigger.vm:8025",
            "title": "Seting up mail"
        },
        {
            "location": "/common-tasks/setting-up-mail/#setting-up-mail",
            "text": "Email is something that many projects need, but during development you likely do \nnot want to actually send email, but you'd rather have sent mail captured for examination\nand released to a real mail server only in certain situations.  To handle these and other situations we recommend  Mailhog",
            "title": "Setting Up Mail"
        },
        {
            "location": "/common-tasks/setting-up-mail/#using-mailhog",
            "text": "MailHog can be added as another service with your projects Docker Compose file.  See the mail service defined in the  Outrigger Example Mail Project \nthat service can be copied into your projects  docker-compose.yml  file, or kept \nseparate and started as needed.  Using the configuration from the example you would configure your application \nto use  mail.outrigger.vm:1025  as your SMTP server and you could view captured \nmail (and choose to release it) via the web interface accessible at  http://mail.outrigger.vm:8025",
            "title": "Using MailHog"
        },
        {
            "location": "/common-tasks/dns-resolution/",
            "text": "DNS Resolution\n\n\nDNS Names for your containers\n\n\nWithin the \nlabels\n section of the docker-compose file you can specify labels that will control the DNS name of your \ncontainers.  DNS names are in the format of \n[name].[image].vm\n (e.g web.outrigger.vm)  \n\n\n\n\n\n\ncom.dnsdock.name\n - This is the type of container. Usually something like web, db, cache, etc.\n\n\n\n\n\n\ncom.dnsdock.image\n - This is generally your project name (e.g outrigger, drupal, etc.)\n\n\n\n\n\n\n\n\nNote\n\n\nAll DNS names of Outrigger containers will end in \n.vm\n\n\n\n\nIf you need multiple domains mapped to a container you can use the \ncom.dnsdock.alias\n setting. It takes a full domain \nname and can take a comma separated list (e.g. otherhost.outrigger.vm or alexandria.phase2.vm). Additionally, dnsdock \nsupports longer domain queries meaning if you have a service named web.outrigger.vm, you can also use \nsomething.web.outrigger.vm and it will resolve to the same IP address.\n\n\nFor all other DNS configuration options, please see the \ndnsdock label documentation\n\n\nDNS Forwarders\n\n\nOutrigger can be configured to use additional name servers to forward DNS requests if the record cannot be resolved by dnsdock.\n\n\nAn example is if you connect to a VPN and need to resolve addresses to private servers within a VPN.  To enable this you \nneed to configure the \nRIG_NAMESERVERS\n environment variable to a comma separated list of \nip:port\n \n(example: \n10.10.7.2:53,8.8.8.8:53\n) before running either \nrig start\n or \nrig dns\n. We suggest putting this \nenv var configuration in your \n~/.bashrc\n or \n~/.zshrc\n so that it is always present when needed.  If you just need this \ntemporarily, you can pass the configuration in with the \n--nameservers\n command line option to \nrig start\n or \nrig dns\n. \n\n\nThis configuration will try each forwarder name server, in order, to resolve names until success or all name servers \nhave been exhausted.\n\n\nDNS Command\n\n\nThere is also a \nrig dns\n command that will launch and configure our DNS services on any Docker Host. If you want to \nconfigure DNS on a Docker Host other than \ndev\n be sure to specify the \n--name\n parameter on \nrig\n.\n\n\nDNS Debugging\n\n\nIf you want to see what containers have registered names, use the \nrig dns-records\n command. This command will list \nall registered container names and aliases along with the container's IP address.",
            "title": "DNS Resolution"
        },
        {
            "location": "/common-tasks/dns-resolution/#dns-resolution",
            "text": "",
            "title": "DNS Resolution"
        },
        {
            "location": "/common-tasks/dns-resolution/#dns-names-for-your-containers",
            "text": "Within the  labels  section of the docker-compose file you can specify labels that will control the DNS name of your \ncontainers.  DNS names are in the format of  [name].[image].vm  (e.g web.outrigger.vm)      com.dnsdock.name  - This is the type of container. Usually something like web, db, cache, etc.    com.dnsdock.image  - This is generally your project name (e.g outrigger, drupal, etc.)     Note  All DNS names of Outrigger containers will end in  .vm   If you need multiple domains mapped to a container you can use the  com.dnsdock.alias  setting. It takes a full domain \nname and can take a comma separated list (e.g. otherhost.outrigger.vm or alexandria.phase2.vm). Additionally, dnsdock \nsupports longer domain queries meaning if you have a service named web.outrigger.vm, you can also use \nsomething.web.outrigger.vm and it will resolve to the same IP address.  For all other DNS configuration options, please see the  dnsdock label documentation",
            "title": "DNS Names for your containers"
        },
        {
            "location": "/common-tasks/dns-resolution/#dns-forwarders",
            "text": "Outrigger can be configured to use additional name servers to forward DNS requests if the record cannot be resolved by dnsdock.  An example is if you connect to a VPN and need to resolve addresses to private servers within a VPN.  To enable this you \nneed to configure the  RIG_NAMESERVERS  environment variable to a comma separated list of  ip:port  \n(example:  10.10.7.2:53,8.8.8.8:53 ) before running either  rig start  or  rig dns . We suggest putting this \nenv var configuration in your  ~/.bashrc  or  ~/.zshrc  so that it is always present when needed.  If you just need this \ntemporarily, you can pass the configuration in with the  --nameservers  command line option to  rig start  or  rig dns .   This configuration will try each forwarder name server, in order, to resolve names until success or all name servers \nhave been exhausted.",
            "title": "DNS Forwarders"
        },
        {
            "location": "/common-tasks/dns-resolution/#dns-command",
            "text": "There is also a  rig dns  command that will launch and configure our DNS services on any Docker Host. If you want to \nconfigure DNS on a Docker Host other than  dev  be sure to specify the  --name  parameter on  rig .",
            "title": "DNS Command"
        },
        {
            "location": "/common-tasks/dns-resolution/#dns-debugging",
            "text": "If you want to see what containers have registered names, use the  rig dns-records  command. This command will list \nall registered container names and aliases along with the container's IP address.",
            "title": "DNS Debugging"
        },
        {
            "location": "/common-tasks/upgrading-rig/",
            "text": "Upgrading Outrigger\n\n\nUpgrade Outrigger CLI, rig\n\n\nHomebrew\n\n\nIf you installed rig via Homebrew, you can check for updates by running\n\n\nbrew update\nbrew upgrade phase2/rig\n\n\n\n\nTesting your installation\n\n\nRun \nrig doctor\n to confirm the system is in good working order.\n\n\nUpgrading your Docker Host\n\n\nIf your \nbrew upgrade\n happens to also update docker, when running \nrig doctor\n you \nmay get a warning about an incompatible Docker version in your Docker Host VM. In that case use the\n\nrig upgrade\n command to upgrade your Docker Host VM to a compatible version.",
            "title": "Upgrading Rig"
        },
        {
            "location": "/common-tasks/upgrading-rig/#upgrading-outrigger",
            "text": "",
            "title": "Upgrading Outrigger"
        },
        {
            "location": "/common-tasks/upgrading-rig/#upgrade-outrigger-cli-rig",
            "text": "",
            "title": "Upgrade Outrigger CLI, rig"
        },
        {
            "location": "/common-tasks/upgrading-rig/#homebrew",
            "text": "If you installed rig via Homebrew, you can check for updates by running  brew update\nbrew upgrade phase2/rig",
            "title": "Homebrew"
        },
        {
            "location": "/common-tasks/upgrading-rig/#testing-your-installation",
            "text": "Run  rig doctor  to confirm the system is in good working order.",
            "title": "Testing your installation"
        },
        {
            "location": "/common-tasks/upgrading-rig/#upgrading-your-docker-host",
            "text": "If your  brew upgrade  happens to also update docker, when running  rig doctor  you \nmay get a warning about an incompatible Docker version in your Docker Host VM. In that case use the rig upgrade  command to upgrade your Docker Host VM to a compatible version.",
            "title": "Upgrading your Docker Host"
        },
        {
            "location": "/common-tasks/multiple-docker-machines/",
            "text": "Multiple Docker Machines\n\n\nThere are special cases in which you may want to have multiple Docker Hosts running, or use a Docker Machine from \nanother client/project. One example of this might be to test the performance differences between VirtualBox and xhyve.  \n\n\nFor these cases, \nrig\n takes a \n--name\n flag that will allow you to specify name of the Docker Host. For all commands \nthis name defaults to \ndev\n but you can configure this name if needed. Also, if you don't want to specify the Docker \nHost name on every command you can specify the \nRIG_ACTIVE_MACHINE\n environment variable to be Docker Machine name of \nthe VM you care to interact with.  This way you can set it once and forget about it.  Unless otherwise specified, the \nname defaults to \ndev\n for all commands.",
            "title": "Multiple Docker Machines"
        },
        {
            "location": "/common-tasks/multiple-docker-machines/#multiple-docker-machines",
            "text": "There are special cases in which you may want to have multiple Docker Hosts running, or use a Docker Machine from \nanother client/project. One example of this might be to test the performance differences between VirtualBox and xhyve.    For these cases,  rig  takes a  --name  flag that will allow you to specify name of the Docker Host. For all commands \nthis name defaults to  dev  but you can configure this name if needed. Also, if you don't want to specify the Docker \nHost name on every command you can specify the  RIG_ACTIVE_MACHINE  environment variable to be Docker Machine name of \nthe VM you care to interact with.  This way you can set it once and forget about it.  Unless otherwise specified, the \nname defaults to  dev  for all commands.",
            "title": "Multiple Docker Machines"
        },
        {
            "location": "/common-tasks/routine-image-maintenance/",
            "text": "Routine Image Maintenance\n\n\nRestart rig on a regular basis to minimize the risk of filesystem or other performance problems with your containers.\n\n\nDocker Image Updates\n\n\nPeriodically update your Docker Images to ensure you have the correct configuration. This should be done in coordination\nwith members of your project team so you are all working from the same version of image. Ideally all images in compose \nfiles are working off of tags to ensure consistency.  Images should be pulled frequently though especially for images\nthat are tagged with a language version, like \nphp70\n. Updates may go into those images and keep the same tag, so pulling\nregularly (and coordinating) will ensure you are getting state of the art.\n\n\nIf your project uses \"loosely versioned\" Docker images (such as specifying \nlatest\n as an image version tag or no tag\nat all which will default to \nlatest\n) your local update schedule should be coordinated with updates to other environments\nand team members.\n\n\nHere is how to update your images\n\n\n# Stop your containers in case they are running.\ndocker-compose stop\n\n# Pull the latest changes for all images referenced in docker-compose.yml\ndocker-compose pull\n\n# Pull the latest changes for all images referenced in build.yml.\ndocker-compose -f build.yml pull\n\n# Remove your old containers to make sure you're using the latest images.\ndocker-compose rm\n\n# Re-create your containers based on the freshly updated images.\ndocker-compose up -d\n\n\n\n\nUpdates for a Specified Image\n\n\ndocker pull outrigger/build:php70\n\n\n\n\nIf either form of \npull\n command fails, try re-running with the \n--no-cache\n option.",
            "title": "Routine Image Maintenance"
        },
        {
            "location": "/common-tasks/routine-image-maintenance/#routine-image-maintenance",
            "text": "Restart rig on a regular basis to minimize the risk of filesystem or other performance problems with your containers.",
            "title": "Routine Image Maintenance"
        },
        {
            "location": "/common-tasks/routine-image-maintenance/#docker-image-updates",
            "text": "Periodically update your Docker Images to ensure you have the correct configuration. This should be done in coordination\nwith members of your project team so you are all working from the same version of image. Ideally all images in compose \nfiles are working off of tags to ensure consistency.  Images should be pulled frequently though especially for images\nthat are tagged with a language version, like  php70 . Updates may go into those images and keep the same tag, so pulling\nregularly (and coordinating) will ensure you are getting state of the art.  If your project uses \"loosely versioned\" Docker images (such as specifying  latest  as an image version tag or no tag\nat all which will default to  latest ) your local update schedule should be coordinated with updates to other environments\nand team members.  Here is how to update your images  # Stop your containers in case they are running.\ndocker-compose stop\n\n# Pull the latest changes for all images referenced in docker-compose.yml\ndocker-compose pull\n\n# Pull the latest changes for all images referenced in build.yml.\ndocker-compose -f build.yml pull\n\n# Remove your old containers to make sure you're using the latest images.\ndocker-compose rm\n\n# Re-create your containers based on the freshly updated images.\ndocker-compose up -d",
            "title": "Docker Image Updates"
        },
        {
            "location": "/common-tasks/routine-image-maintenance/#updates-for-a-specified-image",
            "text": "docker pull outrigger/build:php70  If either form of  pull  command fails, try re-running with the  --no-cache  option.",
            "title": "Updates for a Specified Image"
        },
        {
            "location": "/common-tasks/creating-your-own-images/",
            "text": "Creating your own images\n\n\nCreating a Dockerfile\n\n\nThe Dockerfile you create for your project should represent the application as it will need to run in production. There \nare many reasons to have a Dockerfile to create an image, but if you don't intend on running containers in production \nyou can likely skip this part. \n\n\nThe most minimal project container will need to have your code in it.\n\n\n\n\n\n\nStart by determining which Docker image you will need to extend. \n\n\n\n\n\n\nFor the general Drupal use cases you will base your project Dockerfile on apache-php:php70.\n\n\n\n\nFROM phase2/apache-php:php70\n\n\n\n\n\n\n\n\nThen copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site \ncode into the container docroot\n\n\n\n\nCOPY ./html /var/www/html/\n\n\n\n\n\n\n\n\nBuild the Dockerfile into an image\n\n\n\n\ndocker build -t \nsome-name\n .\n\n\n\n\n\n\n\n\nOn successful build, test the image by running\n\n\n\n\ndocker run -t \nsome-name\n\n\n\n\n\n\n\n\nHere is the full (simple) Dockerfile\n\n\nFROM phase2/apache-php:php70\n\n# Copy in the site\nCOPY ./html /var/www/html/",
            "title": "Creating your own images"
        },
        {
            "location": "/common-tasks/creating-your-own-images/#creating-your-own-images",
            "text": "",
            "title": "Creating your own images"
        },
        {
            "location": "/common-tasks/creating-your-own-images/#creating-a-dockerfile",
            "text": "The Dockerfile you create for your project should represent the application as it will need to run in production. There \nare many reasons to have a Dockerfile to create an image, but if you don't intend on running containers in production \nyou can likely skip this part.   The most minimal project container will need to have your code in it.    Start by determining which Docker image you will need to extend.     For the general Drupal use cases you will base your project Dockerfile on apache-php:php70.   FROM phase2/apache-php:php70     Then copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site \ncode into the container docroot   COPY ./html /var/www/html/     Build the Dockerfile into an image   docker build -t  some-name  .     On successful build, test the image by running   docker run -t  some-name     Here is the full (simple) Dockerfile  FROM phase2/apache-php:php70\n\n# Copy in the site\nCOPY ./html /var/www/html/",
            "title": "Creating a Dockerfile"
        },
        {
            "location": "/faq/general/",
            "text": "General FAQ\n\n\nHow do I configure my web container / virtual hosts so I can run multiple projects at once from the same web container\n\n\nYou don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a \nserver\n as a unit to \ntreating the \nservice\n as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container \nactive for each of them. This is OK because containers are much lighter weight than full virtual machines, though you do \nwant to be careful about starting too many at once.  Consider that you will have a docker-compose file for each project \nyou are working on, and each docker-compose file represents your application and all the services required to run it.\n\nYou could be running multiple docker-compose applications on a single Docker Host.  Realistically, for the performance \nof your computer you would stop one docker-compose environment before you would run another.\n\n\nHow do I see what containers are running on my Docker Host\n\n\nIf you want to see how many containers are present within your Docker Host VM and check on the status of them just \nrun \ndocker ps\n This will show you the containers running, the image they are based on, the ports they expose and the \nname of the container.\n\n\nTo see all the containers on the Docker Host both running and stopped use the command  \ndocker ps -a\n\n\nWhy do I have so many containers / cleanup\n\n\nAny time you finish a project or you need to \nreset\n things, you should clean up your containers for a project by running \n\ndocker-compose rm\n. That will remove the instances for the containers specified in your docker compose file.\n\n\nAdditionally you can run \nrig prune\n to clear out all stopped containers and any dangling images.\n\n\nIs there any regular maintenance needed\n\n\nYes, to keep your environment working smoothly and with current infrastructure configurations, implement a personal \nregimen of \nRoutine Image Maintenance\n.\n\n\nWorking with multiple Docker versions [Homebrew]\n\n\nIf you have multiple versions installed, doing \nbrew info docker-compose\n will list the versions you have installed. The \none with the \n\"*\"\n will be the version that is active.\n\n\nIf you need to switch versions, use the \nbrew switch\n command and run any additional commands like \nbrew unlink ...\n. \n(The \nswitch\n command will provide next command steps if you need to link or unlink any formulae).\n\n\nExample:\n\n\nbrew switch docker 1.13.1\n\n\n\n\nCan I get .vm container names a Docker Host not created with Outrigger\n\n\nYes, with an important caveat that containers will not be able to resolve .vm addresses without reconfiguring the daemon.\n\n\nThe \nrig dns\n command can accept a \n--name\n parameter to run the DNS services on an existing Docker Host\n\n\nWhen we start a machine with \nrig start\n, we do the following things:\n\n\n\n\nStart a machine with the \n-dns=172.17.0.1\n Docker daemon option set so that all containers will try dnsdock for DNS\n\n\nRun dnsdock bound to \n172.17.0.1:53\n to provide .vm addresses for all containers\n\n\nSet up Mac OS X to route 172.17.0.1/16 to the Docker virtual machine's IP so the host machine can access containers direct\n\n\nSet up \n/etc/resolver/vm\n so that OS X will look up .vm addresses through DNS queries to \n172.17.0.1\n\n\n\n\nIf you run \nrig --name=$DOCKER_MACHINE_NAME dns\n, every step except the first will run, so your OS X host will be \nable to reach containers by their .vm addresses but other containers will not. This is the major difference between \ncreating a machine with \nrig start\n and applying the DNS configuration to an existing machine with \nrig dns\n.\n\n\nMonitoring of containers\n\n\nIf you need some insight into how many resources a given Docker container may be using, take advantage of the command \n\ndocker stats \ncontainer name\n.  This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O.\n\nIt is rudimentary but can be very useful in the first line of inspection on a container.",
            "title": "General"
        },
        {
            "location": "/faq/general/#general-faq",
            "text": "",
            "title": "General FAQ"
        },
        {
            "location": "/faq/general/#how-do-i-configure-my-web-container-virtual-hosts-so-i-can-run-multiple-projects-at-once-from-the-same-web-container",
            "text": "You don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a  server  as a unit to \ntreating the  service  as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container \nactive for each of them. This is OK because containers are much lighter weight than full virtual machines, though you do \nwant to be careful about starting too many at once.  Consider that you will have a docker-compose file for each project \nyou are working on, and each docker-compose file represents your application and all the services required to run it. \nYou could be running multiple docker-compose applications on a single Docker Host.  Realistically, for the performance \nof your computer you would stop one docker-compose environment before you would run another.",
            "title": "How do I configure my web container / virtual hosts so I can run multiple projects at once from the same web container"
        },
        {
            "location": "/faq/general/#how-do-i-see-what-containers-are-running-on-my-docker-host",
            "text": "If you want to see how many containers are present within your Docker Host VM and check on the status of them just \nrun  docker ps  This will show you the containers running, the image they are based on, the ports they expose and the \nname of the container.  To see all the containers on the Docker Host both running and stopped use the command   docker ps -a",
            "title": "How do I see what containers are running on my Docker Host"
        },
        {
            "location": "/faq/general/#why-do-i-have-so-many-containers-cleanup",
            "text": "Any time you finish a project or you need to  reset  things, you should clean up your containers for a project by running  docker-compose rm . That will remove the instances for the containers specified in your docker compose file.  Additionally you can run  rig prune  to clear out all stopped containers and any dangling images.",
            "title": "Why do I have so many containers / cleanup"
        },
        {
            "location": "/faq/general/#is-there-any-regular-maintenance-needed",
            "text": "Yes, to keep your environment working smoothly and with current infrastructure configurations, implement a personal \nregimen of  Routine Image Maintenance .",
            "title": "Is there any regular maintenance needed"
        },
        {
            "location": "/faq/general/#working-with-multiple-docker-versions-homebrew",
            "text": "If you have multiple versions installed, doing  brew info docker-compose  will list the versions you have installed. The \none with the  \"*\"  will be the version that is active.  If you need to switch versions, use the  brew switch  command and run any additional commands like  brew unlink ... . \n(The  switch  command will provide next command steps if you need to link or unlink any formulae).  Example:  brew switch docker 1.13.1",
            "title": "Working with multiple Docker versions [Homebrew]"
        },
        {
            "location": "/faq/general/#can-i-get-vm-container-names-a-docker-host-not-created-with-outrigger",
            "text": "Yes, with an important caveat that containers will not be able to resolve .vm addresses without reconfiguring the daemon.  The  rig dns  command can accept a  --name  parameter to run the DNS services on an existing Docker Host  When we start a machine with  rig start , we do the following things:   Start a machine with the  -dns=172.17.0.1  Docker daemon option set so that all containers will try dnsdock for DNS  Run dnsdock bound to  172.17.0.1:53  to provide .vm addresses for all containers  Set up Mac OS X to route 172.17.0.1/16 to the Docker virtual machine's IP so the host machine can access containers direct  Set up  /etc/resolver/vm  so that OS X will look up .vm addresses through DNS queries to  172.17.0.1   If you run  rig --name=$DOCKER_MACHINE_NAME dns , every step except the first will run, so your OS X host will be \nable to reach containers by their .vm addresses but other containers will not. This is the major difference between \ncreating a machine with  rig start  and applying the DNS configuration to an existing machine with  rig dns .",
            "title": "Can I get .vm container names a Docker Host not created with Outrigger"
        },
        {
            "location": "/faq/general/#monitoring-of-containers",
            "text": "If you need some insight into how many resources a given Docker container may be using, take advantage of the command  docker stats  container name .  This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O. \nIt is rudimentary but can be very useful in the first line of inspection on a container.",
            "title": "Monitoring of containers"
        },
        {
            "location": "/faq/troubleshooting/",
            "text": "Troubleshooting\n\n\nSee the following sections for common problems and ways to solve them.\n\n\nRun doctor\n\n\nRun \nrig doctor\n to determine if your environment is set to run Outrigger.\n\n\nEnsure the environment is setup correctly\n\n\nIt can be useful to ensure everything is in a clean state. The following should ensure that\n\n\n\n\nrig stop\n stops the docker machine and cleans up networking\n\n\neval \"$(rig config)\"\n clears environmental variables docker uses to communicate with the docker host\n\n\nrig start\n starts the docker machine\n\n\neval \"$(rig config)\"\n sets environmental variables docker uses to communicate with the docker host\n\n\n\n\nEnsure your images are up to date\n\n\nFrom time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but \nyou can fetch them when you hear new ones are available.\n\n\ndocker pull imagename\n can be used if you want to update a specific image. For example, if you wanted to make sure you \nhad the latest mariadb you'd run \ndocker pull phase2/mariadb\n.\n\n\ndocker-compose pull\n can be used within a project directory to make sure you've got the latest version of all images \nin the docker-compose.yml file.\n\n\nConfigure Your Shell\n\n\nIf you do not have any containers listed when running \ndocker ps\n or you get an error message like:\n\n\nGet http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory.\n\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n\n* Is your docker daemon up and running?\n\n\n\nOr an error message like:\n\n\nCouldn't connect to Docker daemon - you might need to run `boot2docker up`.\n\n\n\nMake sure your shell has the necessary environment variables by running:\n\n\neval \"$(rig config)\"\n\n\nReset everything\n\n\nIf a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of \nblowing everything away and starting over. Note this is a \nnuclear option\n as even your persistent data area will be \nremoved if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your \nVM first with the scripts provided.\n\n\nTo wipe everything out and start over\n\n\n\n\nFirst backup your existing data (if desired) by running \nrig data-backup\n. This will sync your entire \n/data\n \ndirectory to your host machine.\n\n\nThen you can run \nrig remove\n This removes the broken Docker Host and it\u2019s state, making way for a clean start.\n\n\nNext you can rebuild everything by running \nrig start\n. This will create you new Docker Host.\n\n\nIf you wish to restore the \n/data\n directory that you previously backed up, then run \nrig data-restore\n\n\n\n\nFiles Not Found\n\n\nIf you get messages about files not being found in the container that should be shared from your host computer, check the \nfollowing:\n\n\n\n\nIs the project checked out under the \n/Users\n folder? Only files under the \n/Users\n folder are shared into the \nDocker Host (and thus containers) by default.\n\n\nDoes the \ndocker-compose.yml\n file have a volume mount set up that contains the missing files?\n\n\nGet shell in the Docker container via \ndocker exec -it \ncontainer name\n /bin/bash\n and check if the files are \nshared in the wrong place.\n\n\nGet shell in the Docker Machine with \ndocker-machine ssh dev\n and check if you find the files under /Users.\n\n\n\n\nImage not found\n\n\nIf you encounter the following error about the Docker image not being found when starting a project, it may indicate \nthat the image is private and your Docker client is not logged into a required private Docker Hub repository:\n\n\nPulling repository phase2/privateimage\n\nError: image phase2/privateimage:latest not found\n\n\n\nTo solve this, run \ndocker login\n and provide the relevant credentials.\n\n\nDocker client and server version incompatibilities\n\n\nIf you see an error similar to this:\n\n\nError response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19)\n\n\n\nYou likely need to upgrade your Docker Host to a get Docker client API compatibility. Do that with:\n\n\n`rig upgrade`\n\n\n\nNetwork timed out and can't pull container image\n\n\nExample:\n\n\nPulling cache (phase2/memcache:latest)...\nPulling repository docker.io/phase2/memcache\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy.\n\n\n\n\nTry restarting the Docker Machine: \nrig restart\n\n\nStarted machines may have a new IP address\n\n\nExample:\n\n\nStarted machines may have new IP addresses. You may need to re-run the `docker-machine env` command.\n[WARN] Docker daemon not running! Trying again in 3 seconds.  Try 1 of 10.\n...\n[WARN] Docker daemon not running! Trying again in 3 seconds.  Try 10 of 10.\n[ERROR] Docker daemon failed to start!\n\n\n\n\nThe Docker Host (probably called dev) has it's IP changed. The generated TLS Certs are no longer valid and must be regenerated.\n\n\nPossible causes or relations/patterns:\n\n\n\n\nAnother VM was started in VirtualBox.\n\n\nMachine went to sleep and somehow caused issues with the running VM.\n\n\n\n\nFix:\n\n\nTry running: \ndocker-machine env\n\n\nIf that does not work, you should be able to start the VM directly through docker-machine: \ndocker-machine start dev\n\n\nNext, check the IP / TLS status by running: \ndocker-machine ls\n\n\nThe output will likely report something akin to:\n\n\ndev       -        virtualbox   Running   tcp://192.168.99.100:2376 Unknown\nUnable to query docker version: Get https://192.168.99.100:2376/v1.15/version: x509: certificate is valid for 192.168.99.101, not 192.168.99.100\n\n\n\n\nNow, regenerate the TLS Certs: \ndocker-machine regenerate-certs dev -f\n\nDevtools should now be able to start. Don't forget to run \neval \"$(rig config)\"\n after.\n\n\nContainers Started but Service Not Available\n\n\nYour Docker Host is running, the project's containers are up, and command-line operations work fine. Why can't you view \nthe site in your web browser?\n\n\nDNS Services\n\n\nThe DNS services that Outrigger spins up may not be working. Run \ndocker ps\n and see that you have a dnsdock and dnsmasq container.\n\n\nIf those services are not running, try \nrig dns\n to bring them up, or a full \nrig restart\n if that does not work.\n\n\nrig dns-records\n is also useful to see what containers have registered names.\n\n\nDNS Configuration\n\n\nIt is also possible that the DNS services are running for your environment, but somehow the configuration is wrong. Run \n\nrig dns-records\n and make sure your project has an entry. If not, you may need another restart, or perhaps you are \nmissing \ncom.dnsdock.name\n and \ncom.dnsdock.image\n labels in your docker-compose.yml?\n\n\nSlow-starting Services\n\n\nSome services, such as Apache Solr or Varnish, can take longer to start up than Apache and PHP-FPM. As a result you might \nload the browser so fast that not all services are available, which in the case of a proxy may prevent the page from \nloading at all. Wait a short time and try reloading the page.\n\n\nFailed Health Checks\n\n\nSome services such as Varnish depend on others to operate, and have built-in health checks to verify the other service is operating.\n\n\nIf such a health check fails, there could be two problems:\n\n\n\n\nThe internal DNS routing between Docker containers is broken. Make sure the\nconfiguration of your services is correct.\n\n\nThe dependency (e.g., Apache behind Varnish) is not yet up and running when Varnish performs its checks.\n\n\n\n\nIn either case, you can often repair the problem by performing a clean restart of the broken service.\n\n\ndocker-compose stop proxy\ndocker-compose rm -f proxy\ndocker-compose up -d proxy\n\n\n\n\nYour other services should already be up and functional when this is done, so the health check will not fail on account of (2).\n\n\n\n\nChecking on Varnish Health\n\n\nIf you suspect Varnish may be failing, run \ndocker exec -it [VARNISH_CONTAINER] varnishlog\n and scan for VCL compilation errors.\n\n\n\n\nService Became Non-Responsive\n\n\nSometimes a service locks up. Apache stops serving results, Solr stops indexing or responding to search queries. These \nthings happen on servers all the time. It may even happen more often on Docker, especially since we are now using more \n\"infrastructure\" in our local environments.\n\n\nDocker is meant to easily sandbox these problems from the rest of your machine, and to easily resolve these problems by \nallowing you to dump the problem and start over fresh very easily.\n\n\nHard reset on a service (as describe above), is very much like a power cycle:\n\n\ndocker-compose stop [BROKEN_SERVICE]\ndocker-compose rm -f [BROKEN_SERVICE]\ndocker-compose up -d [BROKEN_SERVICE]\n\n\n\n\nSometimes data for a service is volume mounted from outside the container. This persistence is good when the data is \nhealthy, but can be really bad if the data is part of the problem (e.g., broken lockfiles).\n\n\ndocker-machine ssh [MACHINE_NAME]\nrm -Rf /data/[PROJECT]/[DIRECTORY_FOR_SERVICE]\n\n\n\n\nRemember that your service may have configuration or data in another service, you may need to perform this operation \nagainst multiple containers (e.g., Solr)\n\n\nNFS Conflicts with other Environments\n\n\nRunning the Devtools VM in conjunction with other development environments (such as Vagrant) can sometimes cause conflicts \nwith volume mounts. This is due to the fact that the Devtools VM mounts the entirety of \n/Users\n into the VM on OS X. \nAdditional NFS mounts from other environments that target subdirectories of \n/Users\n will fail.\n\n\nThe easiest workaround is to keep projects that use non-Devtools environments like Vagrant in a directory \noutside\n of \n\n/Users\n, such as \n/opt\n. Alternatively, you can run a single VM at a time and manually clear out \n/etc/exports\n prior \nto switching environments/projects.\n\n\nMigrating Vagrant boxes outside of /Users\n\n\nIf you'd like to move your vagrant boxes outside of your home directory, perform the following steps:\n\n\n\n\nChoose a destination folder.  We'll be using /opt/vms for this example. Insure the destination has enough free space. \nTypically this is not a problem because our Macs are one single partition.\n\n\nMake the new directory, and ensure your userid owns it:  \nsudo mkdir /opt/vms; sudo chown -R userid:userid /opt/vms\n\n\nAdd \nexport VAGRANT_HOME=/opt/vms\n to ~/.bash_profile\n\n\nMove your vagrant folders over to /opt/vms.\n\n\nedit /etc/exports, updating any vagrant mount point to use the new location.  (example: /Users/userid/vagrant/projectx/cms)\n\n\nWhile a reboot isn't necessary, it'll help to make sure nothing is running and using the old mount points.\n\n\ncd into your new vagrant directory, and do a vagrant up.",
            "title": "Troubleshooting"
        },
        {
            "location": "/faq/troubleshooting/#troubleshooting",
            "text": "See the following sections for common problems and ways to solve them.",
            "title": "Troubleshooting"
        },
        {
            "location": "/faq/troubleshooting/#run-doctor",
            "text": "Run  rig doctor  to determine if your environment is set to run Outrigger.",
            "title": "Run doctor"
        },
        {
            "location": "/faq/troubleshooting/#ensure-the-environment-is-setup-correctly",
            "text": "It can be useful to ensure everything is in a clean state. The following should ensure that   rig stop  stops the docker machine and cleans up networking  eval \"$(rig config)\"  clears environmental variables docker uses to communicate with the docker host  rig start  starts the docker machine  eval \"$(rig config)\"  sets environmental variables docker uses to communicate with the docker host",
            "title": "Ensure the environment is setup correctly"
        },
        {
            "location": "/faq/troubleshooting/#ensure-your-images-are-up-to-date",
            "text": "From time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but \nyou can fetch them when you hear new ones are available.  docker pull imagename  can be used if you want to update a specific image. For example, if you wanted to make sure you \nhad the latest mariadb you'd run  docker pull phase2/mariadb .  docker-compose pull  can be used within a project directory to make sure you've got the latest version of all images \nin the docker-compose.yml file.",
            "title": "Ensure your images are up to date"
        },
        {
            "location": "/faq/troubleshooting/#configure-your-shell",
            "text": "If you do not have any containers listed when running  docker ps  or you get an error message like:  Get http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory.\n\n* Are you trying to connect to a TLS-enabled daemon without TLS?\n\n* Is your docker daemon up and running?  Or an error message like:  Couldn't connect to Docker daemon - you might need to run `boot2docker up`.  Make sure your shell has the necessary environment variables by running:  eval \"$(rig config)\"",
            "title": "Configure Your Shell"
        },
        {
            "location": "/faq/troubleshooting/#reset-everything",
            "text": "If a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of \nblowing everything away and starting over. Note this is a  nuclear option  as even your persistent data area will be \nremoved if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your \nVM first with the scripts provided.  To wipe everything out and start over   First backup your existing data (if desired) by running  rig data-backup . This will sync your entire  /data  \ndirectory to your host machine.  Then you can run  rig remove  This removes the broken Docker Host and it\u2019s state, making way for a clean start.  Next you can rebuild everything by running  rig start . This will create you new Docker Host.  If you wish to restore the  /data  directory that you previously backed up, then run  rig data-restore",
            "title": "Reset everything"
        },
        {
            "location": "/faq/troubleshooting/#files-not-found",
            "text": "If you get messages about files not being found in the container that should be shared from your host computer, check the \nfollowing:   Is the project checked out under the  /Users  folder? Only files under the  /Users  folder are shared into the \nDocker Host (and thus containers) by default.  Does the  docker-compose.yml  file have a volume mount set up that contains the missing files?  Get shell in the Docker container via  docker exec -it  container name  /bin/bash  and check if the files are \nshared in the wrong place.  Get shell in the Docker Machine with  docker-machine ssh dev  and check if you find the files under /Users.",
            "title": "Files Not Found"
        },
        {
            "location": "/faq/troubleshooting/#image-not-found",
            "text": "If you encounter the following error about the Docker image not being found when starting a project, it may indicate \nthat the image is private and your Docker client is not logged into a required private Docker Hub repository:  Pulling repository phase2/privateimage\n\nError: image phase2/privateimage:latest not found  To solve this, run  docker login  and provide the relevant credentials.",
            "title": "Image not found"
        },
        {
            "location": "/faq/troubleshooting/#docker-client-and-server-version-incompatibilities",
            "text": "If you see an error similar to this:  Error response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19)  You likely need to upgrade your Docker Host to a get Docker client API compatibility. Do that with:  `rig upgrade`",
            "title": "Docker client and server version incompatibilities"
        },
        {
            "location": "/faq/troubleshooting/#network-timed-out-and-cant-pull-container-image",
            "text": "Example:  Pulling cache (phase2/memcache:latest)...\nPulling repository docker.io/phase2/memcache\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy.  Try restarting the Docker Machine:  rig restart",
            "title": "Network timed out and can't pull container image"
        },
        {
            "location": "/faq/troubleshooting/#started-machines-may-have-a-new-ip-address",
            "text": "Example:  Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.\n[WARN] Docker daemon not running! Trying again in 3 seconds.  Try 1 of 10.\n...\n[WARN] Docker daemon not running! Trying again in 3 seconds.  Try 10 of 10.\n[ERROR] Docker daemon failed to start!  The Docker Host (probably called dev) has it's IP changed. The generated TLS Certs are no longer valid and must be regenerated.  Possible causes or relations/patterns:   Another VM was started in VirtualBox.  Machine went to sleep and somehow caused issues with the running VM.   Fix:  Try running:  docker-machine env  If that does not work, you should be able to start the VM directly through docker-machine:  docker-machine start dev  Next, check the IP / TLS status by running:  docker-machine ls  The output will likely report something akin to:  dev       -        virtualbox   Running   tcp://192.168.99.100:2376 Unknown\nUnable to query docker version: Get https://192.168.99.100:2376/v1.15/version: x509: certificate is valid for 192.168.99.101, not 192.168.99.100  Now, regenerate the TLS Certs:  docker-machine regenerate-certs dev -f \nDevtools should now be able to start. Don't forget to run  eval \"$(rig config)\"  after.",
            "title": "Started machines may have a new IP address"
        },
        {
            "location": "/faq/troubleshooting/#containers-started-but-service-not-available",
            "text": "Your Docker Host is running, the project's containers are up, and command-line operations work fine. Why can't you view \nthe site in your web browser?",
            "title": "Containers Started but Service Not Available"
        },
        {
            "location": "/faq/troubleshooting/#dns-services",
            "text": "The DNS services that Outrigger spins up may not be working. Run  docker ps  and see that you have a dnsdock and dnsmasq container.  If those services are not running, try  rig dns  to bring them up, or a full  rig restart  if that does not work.  rig dns-records  is also useful to see what containers have registered names.",
            "title": "DNS Services"
        },
        {
            "location": "/faq/troubleshooting/#dns-configuration",
            "text": "It is also possible that the DNS services are running for your environment, but somehow the configuration is wrong. Run  rig dns-records  and make sure your project has an entry. If not, you may need another restart, or perhaps you are \nmissing  com.dnsdock.name  and  com.dnsdock.image  labels in your docker-compose.yml?",
            "title": "DNS Configuration"
        },
        {
            "location": "/faq/troubleshooting/#slow-starting-services",
            "text": "Some services, such as Apache Solr or Varnish, can take longer to start up than Apache and PHP-FPM. As a result you might \nload the browser so fast that not all services are available, which in the case of a proxy may prevent the page from \nloading at all. Wait a short time and try reloading the page.",
            "title": "Slow-starting Services"
        },
        {
            "location": "/faq/troubleshooting/#failed-health-checks",
            "text": "Some services such as Varnish depend on others to operate, and have built-in health checks to verify the other service is operating.  If such a health check fails, there could be two problems:   The internal DNS routing between Docker containers is broken. Make sure the\nconfiguration of your services is correct.  The dependency (e.g., Apache behind Varnish) is not yet up and running when Varnish performs its checks.   In either case, you can often repair the problem by performing a clean restart of the broken service.  docker-compose stop proxy\ndocker-compose rm -f proxy\ndocker-compose up -d proxy  Your other services should already be up and functional when this is done, so the health check will not fail on account of (2).   Checking on Varnish Health  If you suspect Varnish may be failing, run  docker exec -it [VARNISH_CONTAINER] varnishlog  and scan for VCL compilation errors.",
            "title": "Failed Health Checks"
        },
        {
            "location": "/faq/troubleshooting/#service-became-non-responsive",
            "text": "Sometimes a service locks up. Apache stops serving results, Solr stops indexing or responding to search queries. These \nthings happen on servers all the time. It may even happen more often on Docker, especially since we are now using more \n\"infrastructure\" in our local environments.  Docker is meant to easily sandbox these problems from the rest of your machine, and to easily resolve these problems by \nallowing you to dump the problem and start over fresh very easily.  Hard reset on a service (as describe above), is very much like a power cycle:  docker-compose stop [BROKEN_SERVICE]\ndocker-compose rm -f [BROKEN_SERVICE]\ndocker-compose up -d [BROKEN_SERVICE]  Sometimes data for a service is volume mounted from outside the container. This persistence is good when the data is \nhealthy, but can be really bad if the data is part of the problem (e.g., broken lockfiles).  docker-machine ssh [MACHINE_NAME]\nrm -Rf /data/[PROJECT]/[DIRECTORY_FOR_SERVICE]  Remember that your service may have configuration or data in another service, you may need to perform this operation \nagainst multiple containers (e.g., Solr)",
            "title": "Service Became Non-Responsive"
        },
        {
            "location": "/faq/troubleshooting/#nfs-conflicts-with-other-environments",
            "text": "Running the Devtools VM in conjunction with other development environments (such as Vagrant) can sometimes cause conflicts \nwith volume mounts. This is due to the fact that the Devtools VM mounts the entirety of  /Users  into the VM on OS X. \nAdditional NFS mounts from other environments that target subdirectories of  /Users  will fail.  The easiest workaround is to keep projects that use non-Devtools environments like Vagrant in a directory  outside  of  /Users , such as  /opt . Alternatively, you can run a single VM at a time and manually clear out  /etc/exports  prior \nto switching environments/projects.",
            "title": "NFS Conflicts with other Environments"
        },
        {
            "location": "/faq/troubleshooting/#migrating-vagrant-boxes-outside-of-users",
            "text": "If you'd like to move your vagrant boxes outside of your home directory, perform the following steps:   Choose a destination folder.  We'll be using /opt/vms for this example. Insure the destination has enough free space. \nTypically this is not a problem because our Macs are one single partition.  Make the new directory, and ensure your userid owns it:   sudo mkdir /opt/vms; sudo chown -R userid:userid /opt/vms  Add  export VAGRANT_HOME=/opt/vms  to ~/.bash_profile  Move your vagrant folders over to /opt/vms.  edit /etc/exports, updating any vagrant mount point to use the new location.  (example: /Users/userid/vagrant/projectx/cms)  While a reboot isn't necessary, it'll help to make sure nothing is running and using the old mount points.  cd into your new vagrant directory, and do a vagrant up.",
            "title": "Migrating Vagrant boxes outside of /Users"
        },
        {
            "location": "/faq/docker-for-mac/",
            "text": "Docker for Mac Support\n\n\nDocker for Mac is a native hypervisor implementation of Docker that does not rely on a virtual machine provided by Docker \nMachine.  It is new with some limitations and potential conflicts with Outrigger. We will highlight the path to a peaceful \ncoexistence. \n\n\nUsing only the Docker for Mac Binaries\n\n\nDocker for Mac provides \ndocker\n and \ndocker-compose\n binaries in \n/usr/local/bin\n.  This install location conflicts with\nthe binaries provided by Homebrew, but Outrigger can use these binaries too.  If you have installed Docker for Mac first, \nyou will get errors when trying to install the \ndocker\n and \ndocker-compose\n binaries via Homebrew. Those errors are fine, \nas long as when all is said and done \ndocker\n, \ndocker-compose\n, and \ndocker-machine\n are all available on your \n$PATH\n.\n\n\nTo use the binaries with your Outrigger VM you simply need to run the \neval \"$(rig config)\"\n command to setup your\nDocker environment.  This will point the \ndocker\n to your VM.  You will need to do this in every shell that you want to \nuse the Outrigger VM.  If you then need to transition back to using the Docker for Mac hypervisor, you will need to unset \nthe Outrigger \ndocker\n configuration. Use \neval \"$(docker-machine env -u)\"\n to unset all \nDOCKER_*\n environment variables. \n\n\nIf you need to do this often, we recommend setting up aliases to set/unset the environment vars.\n\n\nalias re='eval \n$(rig config)\n'\nalias ru='eval \n$(docker-machine env -u)\n'\n\n\n\n\nUsing the Docker for Mac Binaries and Hypervisor\n\n\nTo use the Docker for Mac Hypervisor follow the basic instructions below.\n\n\n\n\nDocker for Mac is less performant (early 2017)\n\n\nThe FUSE driver they use to share your host filesystem into the hypervisor/containers is not nearly as performant as\nthe NFS mount we use with the Outrigger VM.  Therefore, operations that do either a lot of filesystem reading (like \ndirectory scan) or writing (like restoring DB dumps) will take much longer to perform.  It is advisable to use the\nOutrigger VM / VirtualBox for the time being simply for performance reasons.\n\n\n\n\n\n\nNetworking limitation when using the Docker for Mac Hypervisor\n\n\nThere are some \nknown limitations\n\nwith the way networking is implemented with Docker for Mac.  Most notable we can not directly access containers on \ntheir native IP address due to the lack of the docker0 bridge network that exists in the Docker Machine VM implementation.\nThese limitations inhibit the fluid environment that Outrigger enables and as such is not natively supported (yet). We\nhave done our best to highlight the issues and some of our ideas for workarounds below. Let us know how it goes if you\nventure down this path.\n\n\n\n\nSetup /data\n\n\nOutrigger makes a convention out of a \n/data\n directory within the VM.  To provide the same directory to Docker for Mac, \ncreate a \n/data\n directory in the root of your Mac filesystem.  You may need to open up the permissions so the container(s)\ncan write to it.\n\n\nsudo mkdir /data \n sudo chmod 777 /data\n\n\n\n\nOnce you have created that directory, go into Docker for Mac Preferences \n File Sharing and add \n/data\n to the list of \nshares and \nApply \n Restart\n Docker for Mac.  With this \n/data\n directory setup your Compose files should work on either \nthe Docker for Mac Hypervisor or the Outrigger VM.\n\n\nAccessing your Container Services\n\n\nDocker for Mac does not provide a mechanism to route network traffic directly to your containers, they only support \npublishing (binding) ports from your running containers to your host.  This means that all services are accessed on \nlocalhost (127.0.0.1) and share the same port space. (Two web server can't both publish to port 80). The approach we think \nmakes sense (but don't actively support) is:\n\n\n\n\nRun \njwilder/nginx-proxy\n as your only service that binds to port 80\n\n\nStart all of your web containers with the \nVIRTUAL_HOST\n environment variable used by nginx-proxy to specify it's domain name\n\n\nResolve the domain name from the above \nVIRTUAL_HOST\n variable in one of two ways\n\n\nEdit your \n/etc/hosts\n file and point the domain name at \n127.0.0.1\n\n\nUse dnsmasq\n\n\nMake sure dnsdock is not running\n\n\nIn \n/etc/resolver/vm\n have \nnameserver 127.0.0.1\n\n\nRun a dnsmasq container, bind it to port 53, configured to resolve all \n.vm\n addresses to \n127.0.0.1\n\n\n\n\n\n\n\n\n\n\n\n\nUninstalling Docker for Mac\n\n\nSince Docker for Mac and Homebrew both install the Docker binaries in to \n/usr/local/bin\n after you uninstall Docker for Mac \nyour Outrigger environment wont work.  You may see an error like \ncommand not found: docker\n. The Docker for Mac \ninstallation overwrote the Homebrew based Docker installation, but brew still believes they are installed, so you'll \nneed to unlink and re-link.\n\n\nbrew unlink docker \n brew link docker\nbrew unlink docker-compose \n brew link docker-compose\nbrew unlink docker-machine \n brew link docker-machine",
            "title": "Docker for Mac"
        },
        {
            "location": "/faq/docker-for-mac/#docker-for-mac-support",
            "text": "Docker for Mac is a native hypervisor implementation of Docker that does not rely on a virtual machine provided by Docker \nMachine.  It is new with some limitations and potential conflicts with Outrigger. We will highlight the path to a peaceful \ncoexistence.",
            "title": "Docker for Mac Support"
        },
        {
            "location": "/faq/docker-for-mac/#using-only-the-docker-for-mac-binaries",
            "text": "Docker for Mac provides  docker  and  docker-compose  binaries in  /usr/local/bin .  This install location conflicts with\nthe binaries provided by Homebrew, but Outrigger can use these binaries too.  If you have installed Docker for Mac first, \nyou will get errors when trying to install the  docker  and  docker-compose  binaries via Homebrew. Those errors are fine, \nas long as when all is said and done  docker ,  docker-compose , and  docker-machine  are all available on your  $PATH .  To use the binaries with your Outrigger VM you simply need to run the  eval \"$(rig config)\"  command to setup your\nDocker environment.  This will point the  docker  to your VM.  You will need to do this in every shell that you want to \nuse the Outrigger VM.  If you then need to transition back to using the Docker for Mac hypervisor, you will need to unset \nthe Outrigger  docker  configuration. Use  eval \"$(docker-machine env -u)\"  to unset all  DOCKER_*  environment variables.   If you need to do this often, we recommend setting up aliases to set/unset the environment vars.  alias re='eval  $(rig config) '\nalias ru='eval  $(docker-machine env -u) '",
            "title": "Using only the Docker for Mac Binaries"
        },
        {
            "location": "/faq/docker-for-mac/#using-the-docker-for-mac-binaries-and-hypervisor",
            "text": "To use the Docker for Mac Hypervisor follow the basic instructions below.   Docker for Mac is less performant (early 2017)  The FUSE driver they use to share your host filesystem into the hypervisor/containers is not nearly as performant as\nthe NFS mount we use with the Outrigger VM.  Therefore, operations that do either a lot of filesystem reading (like \ndirectory scan) or writing (like restoring DB dumps) will take much longer to perform.  It is advisable to use the\nOutrigger VM / VirtualBox for the time being simply for performance reasons.    Networking limitation when using the Docker for Mac Hypervisor  There are some  known limitations \nwith the way networking is implemented with Docker for Mac.  Most notable we can not directly access containers on \ntheir native IP address due to the lack of the docker0 bridge network that exists in the Docker Machine VM implementation.\nThese limitations inhibit the fluid environment that Outrigger enables and as such is not natively supported (yet). We\nhave done our best to highlight the issues and some of our ideas for workarounds below. Let us know how it goes if you\nventure down this path.",
            "title": "Using the Docker for Mac Binaries and Hypervisor"
        },
        {
            "location": "/faq/docker-for-mac/#setup-data",
            "text": "Outrigger makes a convention out of a  /data  directory within the VM.  To provide the same directory to Docker for Mac, \ncreate a  /data  directory in the root of your Mac filesystem.  You may need to open up the permissions so the container(s)\ncan write to it.  sudo mkdir /data   sudo chmod 777 /data  Once you have created that directory, go into Docker for Mac Preferences   File Sharing and add  /data  to the list of \nshares and  Apply   Restart  Docker for Mac.  With this  /data  directory setup your Compose files should work on either \nthe Docker for Mac Hypervisor or the Outrigger VM.",
            "title": "Setup /data"
        },
        {
            "location": "/faq/docker-for-mac/#accessing-your-container-services",
            "text": "Docker for Mac does not provide a mechanism to route network traffic directly to your containers, they only support \npublishing (binding) ports from your running containers to your host.  This means that all services are accessed on \nlocalhost (127.0.0.1) and share the same port space. (Two web server can't both publish to port 80). The approach we think \nmakes sense (but don't actively support) is:   Run  jwilder/nginx-proxy  as your only service that binds to port 80  Start all of your web containers with the  VIRTUAL_HOST  environment variable used by nginx-proxy to specify it's domain name  Resolve the domain name from the above  VIRTUAL_HOST  variable in one of two ways  Edit your  /etc/hosts  file and point the domain name at  127.0.0.1  Use dnsmasq  Make sure dnsdock is not running  In  /etc/resolver/vm  have  nameserver 127.0.0.1  Run a dnsmasq container, bind it to port 53, configured to resolve all  .vm  addresses to  127.0.0.1",
            "title": "Accessing your Container Services"
        },
        {
            "location": "/faq/docker-for-mac/#uninstalling-docker-for-mac",
            "text": "Since Docker for Mac and Homebrew both install the Docker binaries in to  /usr/local/bin  after you uninstall Docker for Mac \nyour Outrigger environment wont work.  You may see an error like  command not found: docker . The Docker for Mac \ninstallation overwrote the Homebrew based Docker installation, but brew still believes they are installed, so you'll \nneed to unlink and re-link.  brew unlink docker   brew link docker\nbrew unlink docker-compose   brew link docker-compose\nbrew unlink docker-machine   brew link docker-machine",
            "title": "Uninstalling Docker for Mac"
        },
        {
            "location": "/faq/how-to-contribute/",
            "text": "How to Contribute\n\n\nThe details are TBD, but you can start by collaborating with us at \nhttps://github.com/phase2/rig",
            "title": "How to contribute"
        },
        {
            "location": "/faq/how-to-contribute/#how-to-contribute",
            "text": "The details are TBD, but you can start by collaborating with us at  https://github.com/phase2/rig",
            "title": "How to Contribute"
        },
        {
            "location": "/appendix/glossary/",
            "text": "Glossary\n\n\nHost Machine\n\n\nYour laptop for the purposes of Outrigger. This is where your project's source code, your IDE, browsers, etc run as well \nas where the virtual machine acting as the Docker Host runs.\n\n\nDocker Host\n\n\nThis is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for \nmultiple projects.\n\n\nDocker Image\n\n\nA read only template that can be instantiated as a running container. An image might supply a service as small as a build \ntool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together \nto build more complex capabilities.\n\n\nContainer\n\n\nA runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on \na single Docker Host and they can be linked so that they they know how to communicate with each other.\n\n\nDocker Machine\n\n\nDocker Machine creates the Docker Host virtual machine in which containers run.  This provides the functionality that we \npreviously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version \nof Linux.  On Macs this tiny version of Linux is called boot2docker.  All of the Docker commands to build and run containers \nwill actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual \nmachine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely \nhave multiple Docker containers running (web server, database, cache, search, etc.).\n\n\nDocker Compose\n\n\nDocker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML \nfile. Each project will have a docker-compose file for each Docker environment that the project supports.  For example, \nthe default docker-compose.yml would be used to start containers for local development, and alternate Docker compose \nfiles could be included for starting containers for the integration/stage and other environments.",
            "title": "Glossary"
        },
        {
            "location": "/appendix/glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/appendix/glossary/#host-machine",
            "text": "Your laptop for the purposes of Outrigger. This is where your project's source code, your IDE, browsers, etc run as well \nas where the virtual machine acting as the Docker Host runs.",
            "title": "Host Machine"
        },
        {
            "location": "/appendix/glossary/#docker-host",
            "text": "This is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for \nmultiple projects.",
            "title": "Docker Host"
        },
        {
            "location": "/appendix/glossary/#docker-image",
            "text": "A read only template that can be instantiated as a running container. An image might supply a service as small as a build \ntool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together \nto build more complex capabilities.",
            "title": "Docker Image"
        },
        {
            "location": "/appendix/glossary/#container",
            "text": "A runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on \na single Docker Host and they can be linked so that they they know how to communicate with each other.",
            "title": "Container"
        },
        {
            "location": "/appendix/glossary/#docker-machine",
            "text": "Docker Machine creates the Docker Host virtual machine in which containers run.  This provides the functionality that we \npreviously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version \nof Linux.  On Macs this tiny version of Linux is called boot2docker.  All of the Docker commands to build and run containers \nwill actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual \nmachine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely \nhave multiple Docker containers running (web server, database, cache, search, etc.).",
            "title": "Docker Machine"
        },
        {
            "location": "/appendix/glossary/#docker-compose",
            "text": "Docker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML \nfile. Each project will have a docker-compose file for each Docker environment that the project supports.  For example, \nthe default docker-compose.yml would be used to start containers for local development, and alternate Docker compose \nfiles could be included for starting containers for the integration/stage and other environments.",
            "title": "Docker Compose"
        },
        {
            "location": "/appendix/architecture/",
            "text": "Architecture\n\n\nOutrigger uses a containerization based approach to providing a project environment. Project services running as containers\nare combined with configuration for persistent data storage, DNS services and network manipulation which allows for easy\naccess via names like service.project.vm rather than IP addresses.\n\n\nThe Outrigger CLI facilitates setting up a VM to act as a container host on systems which can not host containers natively.\nIt also configures the VM with the following:\n\n\n\n\nFilesystem sharing for high performance file access of the host machine\n\n\nContainer initiation for DNS services\n\n\nNetwork and name resolution configuration\n\n\n\n\nLayered with this core are a set of docker images which provide common project services such as a web server along with\nthe ability to configure these services with commonly desired development options. Any docker image can be used if desired.\n\n\nDocker Compose is used to control containers to provide a complete environment and a set of Yeoman based generators are\nused to help initial project setup to ensure all pieces fit together using the current best known options..\n\n\nContainerization\n\n\nIn order to provide lightweight environments and tooling we use a technical approach called containers.\nContainers attempt to move the unit of environment from server to application. This allows separation of concerns between\nhow an application is configured, how the containers communicate with each other, and where the containers are deployed.\nTake an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run\nin its own container, the containers can all run on a single VM for local development and be spread across multiple\nservers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it\nconceptually fits together.\n\n\nDocker\n\n\nThe container implementation we use is called Docker which is explained in the intro: \nWhat is Docker\n?\nThis is the way we capture environment units for our application/services and share them with everyone on the team.\nEnvironments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same\nset of files. For example, nearly every project needs a web server so we have a container image that can be run\nto provide that service. Our Docker images are also set up to allow for configuration adjustment to enable common\ndevelopment options.\n\n\nrig\n\n\nThis is a project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find\nrig in a GitHub repository \nhere\n. There are 2 basic services that rig provides:\n\n\nManage virtual machines for running containers\n\n\nThe rig binary will manage the creation/configuration/upgrade/start/stop of boot2docker virtual machines (a.k.a\nDocker Hosts) via docker-machine. It ensures that the docker-machine virtual machine is the right version, is named\ncorrectly and configured to run efficiently within VirtualBox, VMWare Fusion or xhyve.\n\n\nNice DNS names and routing for running containers\n\n\nOnce there is a safe environment to run our containers we need a way to route traffic to them and provide easy to\nuse/remember domain names to make accessing these services simple. Domain names for containers are set in the\ndocker-compose YAML files using configuration that the \ndnsdock\n container reads to create a mapping between domain\nname and container.\n\n\nWe use \ndnsdock\n running as a container within the Docker Host. The \ndnsdock\n service, which listens on\n172.17.0.1:53535. The \ndnsdock\n container resolves the *.vm domain names to the IP addresses of the containers.\n\n\nInternal container names will look like \nweb.openatrium.vm\n. All Outrigger containers will carry the \n.vm\n extension\nfor name resolution. There is additional information in \nDNS Resolution\n.\n\n\nDocker Hub\n\n\nDocker Hub is where container images are stored and retrieved when your local machine does not already have a copy of\nthe requested container image. Docker Hub can be thought of like GitHub or BitBucket, and Docker Hub images can be thought\nof as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service.\n\n\nContainer Configuration\n\n\nIf a container wants to offer configurable options it will document how to control it within the README or via Environment\nvariables in the Dockerfile itself. See our Apache / PHP \nDockerfile\n\nfor an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables\ncan either be passed on the command line when executing a \ndocker run\n command directly, or in the \nenvironment\n section\nof a docker-compose file. Documentation entries within the \nCommon Tasks\n section provide additional approaches to\ncontainer configuration.",
            "title": "Architecture"
        },
        {
            "location": "/appendix/architecture/#architecture",
            "text": "Outrigger uses a containerization based approach to providing a project environment. Project services running as containers\nare combined with configuration for persistent data storage, DNS services and network manipulation which allows for easy\naccess via names like service.project.vm rather than IP addresses.  The Outrigger CLI facilitates setting up a VM to act as a container host on systems which can not host containers natively.\nIt also configures the VM with the following:   Filesystem sharing for high performance file access of the host machine  Container initiation for DNS services  Network and name resolution configuration   Layered with this core are a set of docker images which provide common project services such as a web server along with\nthe ability to configure these services with commonly desired development options. Any docker image can be used if desired.  Docker Compose is used to control containers to provide a complete environment and a set of Yeoman based generators are\nused to help initial project setup to ensure all pieces fit together using the current best known options..",
            "title": "Architecture"
        },
        {
            "location": "/appendix/architecture/#containerization",
            "text": "In order to provide lightweight environments and tooling we use a technical approach called containers.\nContainers attempt to move the unit of environment from server to application. This allows separation of concerns between\nhow an application is configured, how the containers communicate with each other, and where the containers are deployed.\nTake an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run\nin its own container, the containers can all run on a single VM for local development and be spread across multiple\nservers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it\nconceptually fits together.",
            "title": "Containerization"
        },
        {
            "location": "/appendix/architecture/#docker",
            "text": "The container implementation we use is called Docker which is explained in the intro:  What is Docker ?\nThis is the way we capture environment units for our application/services and share them with everyone on the team.\nEnvironments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same\nset of files. For example, nearly every project needs a web server so we have a container image that can be run\nto provide that service. Our Docker images are also set up to allow for configuration adjustment to enable common\ndevelopment options.",
            "title": "Docker"
        },
        {
            "location": "/appendix/architecture/#rig",
            "text": "This is a project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find\nrig in a GitHub repository  here . There are 2 basic services that rig provides:",
            "title": "rig"
        },
        {
            "location": "/appendix/architecture/#manage-virtual-machines-for-running-containers",
            "text": "The rig binary will manage the creation/configuration/upgrade/start/stop of boot2docker virtual machines (a.k.a\nDocker Hosts) via docker-machine. It ensures that the docker-machine virtual machine is the right version, is named\ncorrectly and configured to run efficiently within VirtualBox, VMWare Fusion or xhyve.",
            "title": "Manage virtual machines for running containers"
        },
        {
            "location": "/appendix/architecture/#nice-dns-names-and-routing-for-running-containers",
            "text": "Once there is a safe environment to run our containers we need a way to route traffic to them and provide easy to\nuse/remember domain names to make accessing these services simple. Domain names for containers are set in the\ndocker-compose YAML files using configuration that the  dnsdock  container reads to create a mapping between domain\nname and container.  We use  dnsdock  running as a container within the Docker Host. The  dnsdock  service, which listens on\n172.17.0.1:53535. The  dnsdock  container resolves the *.vm domain names to the IP addresses of the containers.  Internal container names will look like  web.openatrium.vm . All Outrigger containers will carry the  .vm  extension\nfor name resolution. There is additional information in  DNS Resolution .",
            "title": "Nice DNS names and routing for running containers"
        },
        {
            "location": "/appendix/architecture/#docker-hub",
            "text": "Docker Hub is where container images are stored and retrieved when your local machine does not already have a copy of\nthe requested container image. Docker Hub can be thought of like GitHub or BitBucket, and Docker Hub images can be thought\nof as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service.",
            "title": "Docker Hub"
        },
        {
            "location": "/appendix/architecture/#container-configuration",
            "text": "If a container wants to offer configurable options it will document how to control it within the README or via Environment\nvariables in the Dockerfile itself. See our Apache / PHP  Dockerfile \nfor an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables\ncan either be passed on the command line when executing a  docker run  command directly, or in the  environment  section\nof a docker-compose file. Documentation entries within the  Common Tasks  section provide additional approaches to\ncontainer configuration.",
            "title": "Container Configuration"
        },
        {
            "location": "/appendix/background/",
            "text": "Background\n\n\nThis document covers some of the history of development tooling and explains some of the how and why of the Outrigger approach.\n\n\nWhy Outrigger\n\n\nOutrigger covers an entire toolbox of solutions to accelerate development and smoothly implement best practices. We\nintend to use the Outrigger umbrella to help us roll these tools and approaches out to the whole company in a more\nconsistent fashion.\n\n\nAcross a wide variety of projects and clients, keeping a development, integration, staging, production and other\nenvironments synchronized in terms of server configuration and supporting software versions and tooling has always been\na challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we\nhave used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on\nOpenStack for integration environments and tools like Puppet to try and ensure all of those match each other and the\ndeployment environments.\n\n\nIt\u2019s been better than doing it all by hand but it has required building additional expertise above typical system\nmanagement tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the\nvarious, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address.\n\n\nOutrigger is going to address this problem by providing an efficient way to have project/app-specific environments as well\nas development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools\nlevel rather than at a server level and make the details of our hosting implementation more transparent to all technical\nteam members.\n\n\nMapping Concepts From Previous Approaches\n\n\nThere are some important pieces to remember about how containerization is similar and different to concepts you may be\nfamiliar with.\n\n\nVagrant\n\n\nVagrant is being replaced by the combination of Docker Compose and Docker Machine.\n\n\nVirtual Machines\n\n\nFor local development, you should no longer worry about Virtual Machines at a project level as they are replaced by a\nsingle Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration\ntools like \nKubernetes\n or \nSwarm\n may be used\nto distribute and coordinate containers across multiple Docker Hosts.\n\n\nPuppet / Ansible / configuration management\n\n\nGoodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than\nPuppet. It\u2019s perfectly possible to use tools like Puppet and Ansible from within a Dockerfile to get a container image\nprepared though typically simple shell commands are preferred. The idea is that systems like Puppet are no longer needed\nto manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down\nthe new version of the image to your server and then stop the containers running the old version of the image and start\ncontainers based on the new version of the image.",
            "title": "Background"
        },
        {
            "location": "/appendix/background/#background",
            "text": "This document covers some of the history of development tooling and explains some of the how and why of the Outrigger approach.",
            "title": "Background"
        },
        {
            "location": "/appendix/background/#why-outrigger",
            "text": "Outrigger covers an entire toolbox of solutions to accelerate development and smoothly implement best practices. We\nintend to use the Outrigger umbrella to help us roll these tools and approaches out to the whole company in a more\nconsistent fashion.  Across a wide variety of projects and clients, keeping a development, integration, staging, production and other\nenvironments synchronized in terms of server configuration and supporting software versions and tooling has always been\na challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we\nhave used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on\nOpenStack for integration environments and tools like Puppet to try and ensure all of those match each other and the\ndeployment environments.  It\u2019s been better than doing it all by hand but it has required building additional expertise above typical system\nmanagement tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the\nvarious, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address.  Outrigger is going to address this problem by providing an efficient way to have project/app-specific environments as well\nas development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools\nlevel rather than at a server level and make the details of our hosting implementation more transparent to all technical\nteam members.",
            "title": "Why Outrigger"
        },
        {
            "location": "/appendix/background/#mapping-concepts-from-previous-approaches",
            "text": "There are some important pieces to remember about how containerization is similar and different to concepts you may be\nfamiliar with.",
            "title": "Mapping Concepts From Previous Approaches"
        },
        {
            "location": "/appendix/background/#vagrant",
            "text": "Vagrant is being replaced by the combination of Docker Compose and Docker Machine.",
            "title": "Vagrant"
        },
        {
            "location": "/appendix/background/#virtual-machines",
            "text": "For local development, you should no longer worry about Virtual Machines at a project level as they are replaced by a\nsingle Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration\ntools like  Kubernetes  or  Swarm  may be used\nto distribute and coordinate containers across multiple Docker Hosts.",
            "title": "Virtual Machines"
        },
        {
            "location": "/appendix/background/#puppet-ansible-configuration-management",
            "text": "Goodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than\nPuppet. It\u2019s perfectly possible to use tools like Puppet and Ansible from within a Dockerfile to get a container image\nprepared though typically simple shell commands are preferred. The idea is that systems like Puppet are no longer needed\nto manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down\nthe new version of the image to your server and then stop the containers running the old version of the image and start\ncontainers based on the new version of the image.",
            "title": "Puppet / Ansible / configuration management"
        },
        {
            "location": "/appendix/library/",
            "text": "Library\n\n\nThis library is meant to be a listing of resources from the Interwebz to help you understand how to use Outrigger \nand various related technologies.",
            "title": "Library"
        },
        {
            "location": "/appendix/library/#library",
            "text": "This library is meant to be a listing of resources from the Interwebz to help you understand how to use Outrigger \nand various related technologies.",
            "title": "Library"
        }
    ]
}